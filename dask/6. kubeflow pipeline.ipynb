{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a multi-stage KubeFlow Pipeline\n",
    "\n",
    "* **[Acquire](1.%20remote%20archive%20t%20local%20parquet-airlines.ipynb)** airlines data from remote site and save locally as parquet dataset\n",
    "* **[Load dataset](2.%20parquet%20to%20dask%20cluster-airlines.ipynb)** into persistent Dask cluster\n",
    "* Feature engineering and **[data splits](3.%20generate%20train%20and%20test%20sets-airlines.ipynb)** (train/validation/test)\n",
    "* **[Train](4.%20lightgbm%20on%20dask%20cluster.ipynb)** Dask-LGBM classifier\n",
    "* **[Evaluate]()** trained model\n",
    "* Wrap functionality into a KubeFlow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "mlrun.mlconf.dbpath = 'http://mlrun-api:8080'\n",
    "mlrun.mlconf.kfp_image = 'yjbds/mlrun-dask:dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire_job     = mlrun.import_function('yaml/arc_to_parquet.yaml')\n",
    "dask_job        = mlrun.import_function('yaml/parquet-to-dask.yaml')\n",
    "sum_job         = mlrun.import_function('yaml/describe.yaml')\n",
    "split_job       = mlrun.import_function('yaml/splitter-labelencode.yaml')\n",
    "lgbm_job        = mlrun.import_function('yaml/clf_lgbm_dask.yaml')\n",
    "xgb_job         = mlrun.import_function('yaml/clf_xgboost_dask.yaml')\n",
    "\n",
    "jobs = [acquire_job, dask_job, sum_job, split_job, lgbm_job, xgb_job]\n",
    "\n",
    "for job in jobs:\n",
    "    job.apply(mlrun.mount_v3io())\n",
    "    job.deploy(skip_deployed=True, with_mlrun=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f28dde61400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srvfn = mlrun.new_model_server('fdsvr', \n",
    "                               model_class='ClassifierModel', \n",
    "                               filename='/User/repos/demos/dask/model_server.ipynb')\n",
    "\n",
    "srvfn.apply(mlrun.mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(srvfn.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PATH = '/User/repos/demos/dask/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='My Dask training pipeline',\n",
    "    description='Shows how to use mlrun with dask and a booster model.'\n",
    ")\n",
    "def dask_pipeline(\n",
    "    max_depth = [3, 4, 5], \n",
    "    learning_rate = [0.01, 0.1, 0.5],\n",
    "):\n",
    "\n",
    "    acquire_op = acquire_job.as_step(name='acquire', \n",
    "                                  handler='arc_to_parquet',\n",
    "                                  params = {\n",
    "                                      'target_path': '/User/repos/demos/dask/dataset',\n",
    "                                      'name'       : 'airlines.pqt', \n",
    "                                      'key'        : 'airlines',\n",
    "                                      'archive_url': \"https://s3.amazonaws.com/h2o-airlines-unpacked/allyears.csv\",\n",
    "                                      'dataset'    : 'partitions',\n",
    "                                      'part_cols'  : ['Year', 'Month'],\n",
    "                                      'encoding'   : 'latin-1',\n",
    "                                      'inc_cols'   : ['Year','Month','DayofMonth','DayOfWeek', 'CRSDepTime', 'UniqueCarrier', 'ArrDelay', 'Origin', 'Dest', 'Distance'],\n",
    "                                      'dtype'      : {\n",
    "                                          'Distance'   : 'float32',\n",
    "                                          'ArrDelay'   : 'float64',\n",
    "                                          'CRSDepTime' : 'float32'}},\n",
    "                                  outputs=['airlines'], \n",
    "                                  out_path=TARGET_PATH).apply(mlrun.mount_v3io())\n",
    "\n",
    "    todask_op = dask_job.as_step(name='todask',\n",
    "                              handler='parquet_to_dask',\n",
    "                              params={\n",
    "                                  'sample'           : 0.01,\n",
    "                                  'shards'           : 8,\n",
    "                                  'threads_per'      : 8,\n",
    "                                  'memory_limit'     : '5GB',\n",
    "                                  'dask_key'         : 'airlines',\n",
    "                                  'target_path'      : TARGET_PATH},\n",
    "                              inputs = {'parquet_url': acquire_op.outputs['airlines']}, \n",
    "                              outputs=['scheduler'], \n",
    "                              out_path=TARGET_PATH).apply(mlrun.mount_v3io())\n",
    "\n",
    "    sum_op = sum_job.as_step(name='summarize',\n",
    "                             handler='table_summary',\n",
    "                             inputs={'dask_client':todask_op.outputs['scheduler']},\n",
    "                             params={\n",
    "                                 'dask_key'    :  'airlines',\n",
    "                                 'target_path' :  TARGET_PATH,\n",
    "                                 'name'        : 'table-summary.csv',\n",
    "                                 'key'         : 'table-summary'},\n",
    "                             outputs=['table-summary']).apply(mlrun.mount_v3io())\n",
    "    \n",
    "    split_op = split_job.as_step(name='splitter',\n",
    "                                 handler='splitter_labelencode',\n",
    "                                 inputs={'dask_client': todask_op.outputs['scheduler']},\n",
    "                                 params={'dask_key': 'airlines',\n",
    "                                         'label_column': 'ArrDelay',\n",
    "                                         'categories' : ['UniqueCarrier', 'Origin', 'Dest'],\n",
    "                                         'target_path' : TARGET_PATH},\n",
    "                                 outputs=['header', 'test_set'],\n",
    "                                 out_path=TARGET_PATH).apply(mlrun.mount_v3io())\n",
    "    \n",
    "    lgbm_op = lgbm_job.as_step(name='lgbm',\n",
    "                               handler='clf_lgbm_dask',\n",
    "                               params={\n",
    "                                   'train_set'  : ('xtrain', 'ytrain'),\n",
    "                                   'valid_set'  : ('xvalid', 'yvalid'),\n",
    "                                   'target_path': TARGET_PATH,\n",
    "                                   'name'       : 'lgbm-model.pkl',\n",
    "                                   'key'        : 'lgbm-model',\n",
    "                                   'params'     : {\n",
    "                                       'max_depth'        : 3,\n",
    "                                       'learning_rate'    : 0.1,\n",
    "                                       'n_estimators'     : 3,\n",
    "\n",
    "                                       'reg_alpha'        : 0.,\n",
    "                                       'reg_lambda'       : 0.,\n",
    "                                       'random_state'     : 1,\n",
    "                                       'tree_learner'     : 'data',\n",
    "                                       'silent'           : False}},\n",
    "                               inputs={'dask_client': todask_op.outputs['scheduler']},\n",
    "                               outputs=['lgbm-model'], \n",
    "                               out_path=TARGET_PATH).apply(mlrun.mount_v3io())\n",
    "     \n",
    "    xgb_op = xgb_job.as_step(name='xgb',\n",
    "                             handler='clf_xgboost_dask',\n",
    "                             params={\n",
    "                                 'train_set'  : ('xtrain', 'ytrain'),\n",
    "                                 'valid_set'  : ('xvalid', 'yvalid'),\n",
    "                                 'target_path': TARGET_PATH,\n",
    "                                 'name'       : 'xgb-model.pkl',\n",
    "                                 'key'        : 'xgb-model',\n",
    "                                 'params'     : {\n",
    "                                     'max_depth'          : 3, \n",
    "                                     'num_boost_round'    : 3,\n",
    "                                     'eta'                : 1,\n",
    "                                     'objective'          : 'binary:logistic',\n",
    "                                     'eval_metric'        : ['auc', 'ams@0'],\n",
    "                                     'evals'              : [('xvalid', 'yvalid')],\n",
    "                                     'silent'             : False,\n",
    "                                     'verbose_eval'       : True}},\n",
    "                             inputs={'dask_client': todask_op.outputs['scheduler']},\n",
    "                             outputs=['xgb-model'], \n",
    "                             out_path=TARGET_PATH).apply(mlrun.mount_v3io())\n",
    "\n",
    "    lgbm_op.after(split_op) # since both ops use the same inputs they are run in parallel,\n",
    "                            # this ensures sequential execution\n",
    "    \n",
    "    xgb_op.after(split_op)  # since both ops use the same inputs they are run in parallel,\n",
    "                            # this ensures sequential execution\n",
    "    \n",
    "    deploy = srvfn.deploy_step(project='dask', \n",
    "                               models={'dask_v1': lgbm_op.outputs['lgbm-model']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/User/.pythonlibs/jupyter-1/lib/python3.6/site-packages/kfp/components/_data_passing.py:133: UserWarning: Missing type name was inferred as \"JsonArray\" based on the value \"[3, 4, 5]\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/User/.pythonlibs/jupyter-1/lib/python3.6/site-packages/kfp/components/_data_passing.py:133: UserWarning: Missing type name was inferred as \"JsonArray\" based on the value \"[0.01, 0.1, 0.5]\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "# for debug generate the pipeline dsl\n",
    "kfp.compiler.Compiler().compile(dask_pipeline, 'yaml/lgbm-dask-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(namespace='default-tenant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.yjb-ds-3.iguazio-cd2.com/pipelines//#/experiments/details/32605ad0-9944-4f25-89f4-71f06361f730\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.yjb-ds-3.iguazio-cd2.com/pipelines//#/runs/details/79560a6e-b24d-4b17-a1c9-8cddfc884636\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arguments = {}\n",
    "\n",
    "run_result = client.create_run_from_pipeline_func(dask_pipeline, arguments, experiment_name='dask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
