{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "import os\n",
    "import numpy as np\n",
    "mlrun.mlconf.dbpath = 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters\n",
    "\n",
    "**Please be sure to run the notebooks [1. remote archive to local parquet](1.%20remote%20archive%20to%20local%20parquet.ipynb) and [2. parquet to dask cluster](2.%20parquet%20to%20dask%20cluster.ipynb) before running this one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is already loaded into a Dask cluster we use that as our source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION           = 'train_valid_test'\n",
    "DESCRIPTION        = 'split data into train, validation and test splits'\n",
    "\n",
    "IMAGE              = 'yjbds/mlrun-dask:dev'\n",
    "JOB_KIND           = 'job'\n",
    "TASK_NAME          = 'user-task-data-splits'\n",
    "\n",
    "TARGET_PATH        = '/User/repos/demos/dask/artifacts'\n",
    "DASK_CLIENT        = 'scheduler.json'\n",
    "DASK_KEY           = 'airlines'\n",
    "LABEL_COLUMN       = 'ArrDelay'\n",
    "CATEGORIES         = ['UniqueCarrier', 'Origin', 'Dest']\n",
    "\n",
    "# insert run id ... from db here\n",
    "MLRUN_DB_UID       = '338465d3d0d940e181da7268404db66b'\n",
    "\n",
    "RNG                = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function from a local Python file\n",
    "splitter = mlrun.new_function(command='/User/repos/demos/dask/code/train-valid-test-splitter.py', \n",
    "                              image=IMAGE,\n",
    "                              kind=JOB_KIND)\n",
    "\n",
    "splitter.spec.build.image = IMAGE\n",
    "\n",
    "# export or load function yaml\n",
    "splitter.export('/User/repos/demos/dask/yaml/train-valid-test-splitter.yaml')\n",
    "# splitter = mlrun.import_function('/User/repos/dask/yaml/train_valid_test_splitter-airlines.yaml')\n",
    "\n",
    "splitter.apply(mlrun.mount_v3io())\n",
    "splitter.deploy(skip_deployed=True, with_mlrun=False)\n",
    "\n",
    "task_ = mlrun.NewTask(\n",
    "    TASK_NAME,\n",
    "    params={\n",
    "        'dask_client'   : DASK_CLIENT,\n",
    "        'dask_key'      : DASK_KEY,\n",
    "        'label_column'  : LABEL_COLUMN,\n",
    "        'categories'    : CATEGORIES,\n",
    "        'target_path'   : TARGET_PATH,\n",
    "        'random_state'  : RNG,\n",
    "    })\n",
    "\n",
    "tsk2 = splitter.run(task_, handler='train_valid_test_splitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(scheduler_file='/User/repos/demos/dask/artifacts/scheduler.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.get_dataset('ytrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('airlines', 'xtrain', 'xvalid', 'ytrain', 'yvalid')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_datasets = client.list_datasets()\n",
    "published_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> unknown </td> <td> unknown </td></tr>\n",
       "    <tr><th> Shape </th><td> (nan,) </td> <td> (nan,) </td></tr>\n",
       "    <tr><th> Count </th><td> 123540 Tasks </td><td> 12354 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> bool </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(nan,), dtype=bool, chunksize=(nan,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
