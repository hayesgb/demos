{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Server**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "## **steps**\n",
    "**[notebook installs](#installs)**<br>\n",
    "**[nuclio code section](#nuclio)**<br>\n",
    "    - [inference server](#server)<br>\n",
    "**[deploy](#deploy)**<br>\n",
    "**[test deployment](#test)**<br>\n",
    "**[test saved model object](#testingoutside)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"installs\"></a>\n",
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **notebook installs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the packages we'll need to run this notebook, please install them once:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    !pip install -U kfserving==0.2.0 tensorflow==2.0.0b1 keras pandas \n",
    "    !pip install -U kubernetes==9.0.0\n",
    "    !pip install -U azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install -U git+https://github.com/yjb-ds/functions-demo.git\n",
    "    !pip install -U git+https://github.com/mlrun/mlrun.git@development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nuclio\"></a>\n",
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **nuclio code section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have never run nuclio functions in your notebooks, please uncomment and run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    !pip install nuclio-jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following packages so they are available to the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "pip install kfserving==0.2.0\n",
    "pip install  numpy==1.16.4 tensorflow==2.0.0b1 pandas==0.25.3\n",
    "pip install -U azure joblib\n",
    "pip install -U git+https://github.com/yjb-ds/functions-demo.git\n",
    "pip install -U git+https://github.com/mlrun/mlrun.git@development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import kfserving\n",
    "import numpy as np\n",
    "import joblib\n",
    "from typing import List\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from functions.models import classifier_gen\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeaturesEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Engineer features from raw input.\n",
    "\n",
    "    A standard transformer mixin that can be inserted into a scikit learn Pipeline.\n",
    "    \n",
    "    To use, \n",
    "    >>> ffg = FeaturesEngineer()\n",
    "    >>> ffg.fit(X)\n",
    "    >>> x_transformed = ffg.transform(X)\n",
    "    or\n",
    "    >>> ffg = FeaturesEngineer()\n",
    "    >>> x_transformed = ffg.fit_transform(X)\n",
    "    \n",
    "    In a pipeline:\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> from sklearn.preprocessing import StandardScaler\n",
    "    >>> transformers = [('feature_gen', FeaturesEngineerFeature()), \n",
    "                        ('scaler', StandardScaler())]\n",
    "    >>> transformer_pipe = Pipeline(transformers)\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"fit is unused here\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Transform raw input data as a preprocessing step.\n",
    "        \n",
    "        :param X: Raw input features\n",
    "        \n",
    "        Returns a DataFrame of features.\n",
    "        \"\"\"\n",
    "        x = X.copy()\n",
    "        \n",
    "        # do some cool feature engineering:here we replace by a N(2,2) series\n",
    "        m = 2.0\n",
    "        s = 2.0\n",
    "        \n",
    "        print(x.shape) # transform only if x is 2D\n",
    "        if len(x.shape) == 2:\n",
    "            print('here')\n",
    "            n, f = x.shape\n",
    "            \n",
    "            if type(x)==np.ndarray:\n",
    "                x[:, f-1] = np.random.normal(m, s, n)\n",
    "            else:\n",
    "                x.values[:, f-1] = np.random.normal(m, s, n)\n",
    "        \n",
    "        x = x.astype('float')\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MyKerasClassifier(kfserving.KFModel):\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 model_dir: str,\n",
    "                 classifier: Sequential = None):\n",
    "        \"\"\"TFKerasClassifier\n",
    "        \n",
    "        KubeFlow serving model wrapper.\n",
    "        \n",
    "        :param name:            model name\n",
    "        :param model_dir: path of stored model\n",
    "        :param classifier:      class type of classifier model\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__(name)\n",
    "        self.name = name\n",
    "        self.model_dir = model_dir\n",
    "        if classifier:\n",
    "            self.ready = True\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load model from KubeFlow storage.\n",
    "        \"\"\"\n",
    "        ffg = FeaturesEngineer()\n",
    "\n",
    "        # scaler\n",
    "        ss = StandardScaler()\n",
    "        ss.__dict__ = joblib.load(f\"{self.model_dir}/scaler.pickle\")\n",
    "\n",
    "        # keras model\n",
    "        ksm = classifier_gen()\n",
    "        ksm.load_weights(f\"{self.model_dir}/weights.h5\")\n",
    "\n",
    "        pipe = make_pipeline(ffg, ss, ksm)\n",
    "\n",
    "        self.classifier = pipe\n",
    "\n",
    "    def predict(self, body):\n",
    "        \"\"\"Generate model predictions from sample.\n",
    "        \n",
    "        :param body: A list of observations, each of which is an 1-dimensional feature vector.\n",
    "            \n",
    "        Returns model predictions as a `List`, one for each row in the `body` input `List`.\n",
    "        \"\"\"\n",
    "        try:\n",
    "#             feats = np.asarray(body['instances'])\n",
    "#             result: np.ndarray = self.classifier.predict(feats)\n",
    "            return body\n",
    "#             return result.tolist()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to predict {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **deploy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_model_server, mount_v3io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PATH = \"/User/mlrun/simdata\"\n",
    "MODEL_NAME = \"tfkeras_serving_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = new_model_server(MODEL_NAME, \n",
    "                      models={\"tfkeras_joblib_v2\": TARGET_PATH}, \n",
    "                      model_class=\"MyKerasClassifier\").apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-01-01 19:49:19,306 deploy started\n",
      "[nuclio] 2020-01-01 19:49:23,428 (info) Build complete\n",
      "[nuclio] 2020-01-01 19:49:30,540 done updating tfkeras-serving-v2, function address: 3.137.70.243:30007\n"
     ]
    }
   ],
   "source": [
    "#fn.verbose=True\n",
    "addr = fn.deploy(project=\"refactoring-demos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab some data - balanced dataset\n",
    "features = pd.read_csv(\"x_test_50.csv\")\n",
    "labels = pd.read_csv(\"y_test_50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"instances\" : features.values.tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"testingoutside\"></a>\n",
    "##### **testing our model outside the server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> : 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics  import accuracy_score\n",
    "\n",
    "# create and load\n",
    "model = MyKerasClassifier('anyname', model_dir=TARGET_PATH)\n",
    "model.load()\n",
    "\n",
    "y_pred = model.predict(event)\n",
    "print(type(y_pred), ':', len(y_pred['instances']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "params = json.dumps(event).encode('utf8')\n",
    "req = urllib.request.Request(addr + \"/tfkeras_joblib_v2/predict\", data=params, method='PUT', headers={'content-type': 'application/json'})\n",
    "resp = urllib.request.urlopen(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'{\"instances\": []}']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__next__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_checkClosed',\n",
       " '_checkReadable',\n",
       " '_checkSeekable',\n",
       " '_checkWritable',\n",
       " '_check_close',\n",
       " '_close_conn',\n",
       " '_get_chunk_left',\n",
       " '_method',\n",
       " '_peek_chunked',\n",
       " '_read1_chunked',\n",
       " '_read_and_discard_trailer',\n",
       " '_read_next_chunk_size',\n",
       " '_read_status',\n",
       " '_readall_chunked',\n",
       " '_readinto_chunked',\n",
       " '_safe_read',\n",
       " '_safe_readinto',\n",
       " 'begin',\n",
       " 'chunk_left',\n",
       " 'chunked',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'code',\n",
       " 'debuglevel',\n",
       " 'detach',\n",
       " 'fileno',\n",
       " 'flush',\n",
       " 'fp',\n",
       " 'getcode',\n",
       " 'getheader',\n",
       " 'getheaders',\n",
       " 'geturl',\n",
       " 'headers',\n",
       " 'info',\n",
       " 'isatty',\n",
       " 'isclosed',\n",
       " 'length',\n",
       " 'msg',\n",
       " 'peek',\n",
       " 'read',\n",
       " 'read1',\n",
       " 'readable',\n",
       " 'readinto',\n",
       " 'readinto1',\n",
       " 'readline',\n",
       " 'readlines',\n",
       " 'reason',\n",
       " 'seek',\n",
       " 'seekable',\n",
       " 'status',\n",
       " 'tell',\n",
       " 'truncate',\n",
       " 'url',\n",
       " 'version',\n",
       " 'will_close',\n",
       " 'writable',\n",
       " 'write',\n",
       " 'writelines']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"instances\": []}'\n"
     ]
    }
   ],
   "source": [
    "resp = requests.put(addr + \"/tfkeras_joblib_v2/predict\", json=event)\n",
    "\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        module\n",
       "\u001b[0;31mString form:\u001b[0m <module 'requests' from '/conda/lib/python3.6/site-packages/requests/__init__.py'>\n",
       "\u001b[0;31mFile:\u001b[0m        /conda/lib/python3.6/site-packages/requests/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Requests HTTP Library\n",
       "~~~~~~~~~~~~~~~~~~~~~\n",
       "\n",
       "Requests is an HTTP library, written in Python, for human beings. Basic GET\n",
       "usage:\n",
       "\n",
       "   >>> import requests\n",
       "   >>> r = requests.get('https://www.python.org')\n",
       "   >>> r.status_code\n",
       "   200\n",
       "   >>> 'Python is a programming language' in r.content\n",
       "   True\n",
       "\n",
       "... or POST:\n",
       "\n",
       "   >>> payload = dict(key1='value1', key2='value2')\n",
       "   >>> r = requests.post('https://httpbin.org/post', data=payload)\n",
       "   >>> print(r.text)\n",
       "   {\n",
       "     ...\n",
       "     \"form\": {\n",
       "       \"key2\": \"value2\",\n",
       "       \"key1\": \"value1\"\n",
       "     },\n",
       "     ...\n",
       "   }\n",
       "\n",
       "The other HTTP methods are supported - see `requests.api`. Full documentation\n",
       "is at <http://python-requests.org>.\n",
       "\n",
       ":copyright: (c) 2017 by Kenneth Reitz.\n",
       ":license: Apache 2.0, see LICENSE for more details.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "requests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
