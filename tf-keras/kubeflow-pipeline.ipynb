{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tensorflow-Keras and Scikit-Learn With MLRun**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLRun is an open-source Python package that provides a framework for running machine learning tasks transparently in multiple, scalable, runtime environments.  MLRun provides tracking of code, metadata, inputs, outputs and the results of machine learning pipelines. \n",
    "\n",
    "In this notebook we\"ll compose a pipeline that deploys a classifier model, and uses it as the input in a training and validation step. We'll be working with a synthetic features matrix of dimension 10 million rows by 20 features and a binary label.  The model will be a 2-layer neural net classifier using **[tensorflow-keras](https://www.tensorflow.org/)** (v2.0.0b1), without gpu support.\n",
    "\n",
    "The dataset we create is balanced, however there is a `weight` parameter in the data generator function specifying the fraction of observations that are labeled 0/False. The number of samples and features are also parameters.  The demonstration could be modified easily to allow for a more fine-grained control over the simulated dataset either by adding more parameters or replacing the underlying function altogether.\n",
    "\n",
    "The training and validation step employs a scikit learn `Pipeline` to perform feature engineering. Some of the feature engineering needs to be done _**after**_ the train-valid-test set split. In some preprocessing scenarios we might estimate a data transformation on the training set before model training, and then apply the estimate to the validation and test sets before prediction. Since we need to perform the same transformation pre-inference, all pipeline model steps are stored.\n",
    "\n",
    "Serializing models can be challenging for number of reasons:  a pipeline with multiple steps may require just as many encoding and decoding routines--applying joblib to a Keras model that has been wrapped in a scikit-learn api fails.  Since we have the model architecture in a class definition, all we need to do is save the weights.  Some steps in a pipeline may have no internal state to store, while others can be stored and loaded using `joblib`.  Most of it all boils down to storing dicts/json with numpy objects.\n",
    "\n",
    "One of the upsides of the present architecture is that we can mix many simulations of data with a given model estimator, or many models with a given data sample and track everything in **MLRun**.  Research, development, and deployment, all on one page, running under multiple configurations, limited only by the compute resources at our disposal.\n",
    "\n",
    "\n",
    "#### **notebook take-aways**\n",
    "* write and test reusable and replaceable **[MLRun](https://github.com/mlrun)** components in a notebook\n",
    "* store and load models\n",
    "* run the components as a **[KubeFlow](https://www.kubeflow.org/)** pipeline\n",
    "\n",
    "<a id='top'></a>\n",
    "#### **steps**\n",
    "**[install the python mlrun package](#install)**<br>\n",
    "**[nuclio code section](#nuclio-code-section)**<br>\n",
    "    - [nuclio's ignore](#ignore)<br>\n",
    "    - [function dependencies](#function-dependencies)<br>\n",
    "\n",
    "**[components](#components)**<br>\n",
    "    - [supporting functions](#utilties)<br>\n",
    "    - [data simulation](#datasim)<br>\n",
    "    - [feature engineering](#feateng)<br>\n",
    "    - [a classifier](#classifier)<br>\n",
    "    - [training and validation](#train)<br>\n",
    "**[local tests](#local-testing)**<br>\n",
    "**[compose pipeline](#image)**<br>\n",
    "**[run](#run)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **notebook installs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will reinstall the latest development version of ```mlrun```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    !pip uninstall mlrun -y\n",
    "\n",
    "    !pip install -U git+https://github.com/mlrun/mlrun.git@development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the KubeFlow pipelines package ```kfp```. For more information see the **[KubeFlow documentation on nuclio](https://www.kubeflow.org/docs/components/misc/nuclio/)** and  **[Kubeflow pipelines and nuclio](https://github.com/kubeflow/pipelines/tree/master/components/nuclio)**. For logging the estimated machine learning models we'll use ```joblib```'s ```dump``` and ```load```. For more details see **[Joblib: running Python functions as pipeline jobs](https://joblib.readthedocs.io/en/latest/index.html)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install numpy==1.16.4\n",
    "    !pip install -U kfp joblib \n",
    "    !pip install -U seaborn matplotlib \n",
    "    !pip install -U sklearn pandas tensorflow==2.0.0b1\n",
    "    !pip install -U --no-cache-dir git+https://github.com/yjb-ds/functions-demo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nuclio-code-section\"><a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **nuclio code section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ignore'></a>\n",
    "### _nuclio's **ignore** notation_\n",
    "\n",
    "You\"ll write all the code that gets packaged for execution between the tags ```# nuclio: ignore```, meaning ignore all the code here and above, and ```# nuclio: end-code```, meaning ignore everything after this annotation.  The **[docs](https://github.com/nuclio/nuclio-jupyter#creating-and-debugging-functions-using-nuclio-magic)** also suggest another approach: we can use ```# nuclio: start``` at the first relevant code cell instead of marking all the cells above with ```# nuclio: ignore```.\n",
    "\n",
    "See the **[nuclio-jupyter](https://github.com/nuclio/nuclio-jupyter)** repo for further information on these and many other **[nuclio magic commands](https://github.com/nuclio/nuclio-jupyter#creating-and-debugging-functions-using-nuclio-magic)** that make it easy to transform a Jupyter notebook environment into a platform for developing production-quality, machine learning systems.\n",
    "\n",
    "The ```nuclio-jupyter``` package provides methods for automatically generating and deploying nuclio serverless functions from code, repositories or Jupyter notebooks. **_If you have never run nuclio functions in your notebooks, please uncomment and run the following_**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install nuclio-jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two lines _**should be in the same cell**_ and mark the start of your mchine learning coding section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"function-dependencies\"></a>\n",
    "### _function dependencies_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installs made in the section **[Setup](#Setup)** covered the Jupyter environment within which this notebook runs.  However, we need to ensure that all the dependencies our nuclio function relies upon (such as ```matplotlib```, ```sklearn```, ```lightgbm```), will be available when that code is wrapped up into a nuclio function _**on some presently unknown runtime**_.   Within the nuclio code section we can ensure these dependencies get built into the function with the ```%nuclio cmd``` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio cmd -c pip install -U git+https://github.com/yjb-ds/functions-demo.git\n",
    "%nuclio cmd -c pip install matplotlib tensorflow==2.0.0b1 sklearn pandas numpy==1.16.4 joblib\n",
    "%nuclio cmd -c pip install git+https://github.com/mlrun/mlrun.git@development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\"ll use a standard base image here, however the build step can be shortened by preparing images with pre-installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.build.baseImage to 'python:3.6-jessie'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config spec.build.baseImage = \"python:3.6-jessie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _imports_\n",
    "\n",
    "Some of the functionality is provided in supporting components within the ```functions``` folder.<br>\n",
    "\n",
    "Links to the code:\n",
    "- **[datasets](functions/datasets.py)**:&emsp;generate simulation data\n",
    "- **[files](functions/file_fs.py)**:&emsp;&emsp;&emsp;save and load files\n",
    "- **[models](function/model_fs.py)**:&nbsp; &emsp;save, load, and instantiate models\n",
    "- **[plots](functions/plot_fs.py)**:&emsp;  &emsp; sundry plotting functions\n",
    "- **[tables](functions/tables.py)**:&emsp; &nbsp; &nbsp;logging and retrieving table artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.datasets import create_binary_classification\n",
    "from functions.tables import log_context_table, get_context_table\n",
    "from functions.models import FeaturesEngineer, Classifier, class_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Union, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='paths'></a>\n",
    "### _paths and parameters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PATH = '/User/mlrun/simdata'\n",
    "\n",
    "# data simulation and ml training parameter\n",
    "BATCH_SIZE = 1_024\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS= 3\n",
    "N_SAMPLES = 100_000\n",
    "M_FEATURES = 20\n",
    "CLASS_BALANCE = 0.5\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"components\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='delete'></a>\n",
    "## **data generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "    context: MLClientCtx,\n",
    "    samples: int,\n",
    "    features: int,\n",
    "    features_hdr: Optional[List[str]],\n",
    "    neg_weight: float,\n",
    "    target_path: str,\n",
    "    key: str\n",
    ") -> None:\n",
    "    \"\"\"Generate raw data for this pipeline\n",
    "    \n",
    "    This component will be the entry point of the pipeline.\n",
    "    \n",
    "    In this demonstration our component is a simple wrapper for scikit learn's \n",
    "    `make_classification`, a convenient utility enabling us to build\n",
    "    and test a pipeline from start to finish with a clean and \n",
    "    predictable dataset. By fiddling with neg_weight, we can also take a \n",
    "    quick look at the effect of class balance on our model before exposing it\n",
    "    to the kind of data we find in the real world.\n",
    "    \n",
    "    :param context:       function context\n",
    "    :param samples:       number of samples (rows) to generate\n",
    "    :param features:      number of features (cols)\n",
    "    :param features_hdr:  (optional) header for the features array\n",
    "    :param neg_weights:   fraction of negative samples\n",
    "    :param target_path:   destination for data including file name\n",
    "    :param key:           context key of data\n",
    "    \"\"\"\n",
    "    if features_hdr:\n",
    "        assert len(features_hdr)==m_features, f\"features header dimension mismatch for {name}\"\n",
    "    data = create_binary_classification(\n",
    "                context, n_samples=samples, m_features=features,\n",
    "                features_hdr=features_hdr,  weight=neg_weight, \n",
    "                target_path=target_path, key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feateng'></a>\n",
    "## **feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For code please see the custom sklearn transformer `FeaturesEngineer` in **[models.py](functions/models.py)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classifier\"></a>\n",
    "## **classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For code please see `KerasClassifier` and `classifier_gen` in **[models.py](functions/models.py)**.  This method generates a small keras Sequential model with 2 layers which gets wrapped in a `KerasClassifier` class. The latter provides it with a convenient sklearn interface for use in **[sklearn Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn-pipeline-pipeline)**. The list of metrics collected during training can also be found in the same module as `METRICS` and includes accuracy, precision, recall, auc and a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-save'></a>\n",
    "## **save model**\n",
    "\n",
    "The model presented here has three stages, wrapped into an sklearn pipeline.  In order to save this model pipeline, each of its components may have to be saved independently.  There may be other advantages to serializing the pipeline components separately.\n",
    "\n",
    "In our pipeline, the `FeaturesEngineer` has no state so we just create a new one during load. \n",
    "\n",
    "The `StandardScaler`'s estimates need to be re-used when transforming new data, so it is\n",
    "pickled using `joblib` (it contains arrays).  Its filename is `{path.join(target_path,name)}-scaler.pickle`. **It is important to remember that feature stores need more than feature arrays, they need code and often data in the form of parameter estimates, correlation matrices and so on so that these may be used again on new data, and for forensics.**\n",
    "\n",
    "Keras models have an architecture saved as json, and corresponding weights are saved in hdf5 format. The architecture can be recreated by instantiating the class or converting the json representation into a model, with the weights loaded into that structure. Filenames for the model are `{path.join(target_path,name)}-weights.h5` and `{path.join(target_path,name)}-model.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pipeline_save(\n",
    "    context: MLClientCtx,\n",
    "    pipe: Pipeline,\n",
    "    target_path: str,\n",
    "    key: str = 'model'\n",
    "):\n",
    "    \"\"\"Serialize a specific pipeline.\n",
    "    \n",
    "    :param context:         function context\n",
    "    :param pipe:            estimated model pipeline\n",
    "    :param target_path:     destination for saved model\n",
    "    :param name:            model name\n",
    "    \"\"\"\n",
    "    if target_path:\n",
    "        os.makedirs(target_path, exist_ok=True)\n",
    "    \n",
    "    # StandardScaler\n",
    "    joblib.dump(\n",
    "        pipe.steps[1][1].__dict__, \n",
    "        target_path + '/scaler.pickle')\n",
    "    \n",
    "    # Keras model--if we have the class code, we don't need this\n",
    "    json.dump(\n",
    "        pipe.steps[2][1].model.to_json(), \n",
    "        open(f'{target_path}/model.json', 'w'))\n",
    "    pipe.steps[2][1].model.save_weights(f'{target_path}/weights.h5')\n",
    "    \n",
    "    context.log_artifact(key, \n",
    "                         target_path=target_path, \n",
    "                         labels={\"engineer\": \"functions.models.FeaturesEngineer\",\n",
    "                                 \"scaler\": \"sklearn.preprocessing.StandardScaler\",\n",
    "                                 'model': 'tensorflow.keras.sklearn.KerasClassifier',\n",
    "                                 \"type\": \"classifier\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pipeline_load(\n",
    "    target_path: str,\n",
    "    key: str\n",
    ") -> Pipeline:\n",
    "    \"\"\"Deserialize a specific pipeline.\n",
    "    \n",
    "    See 'pipeline_save' for details.\n",
    "    \n",
    "    :param target_path: location of saved model + prefix of file names\n",
    "                        For example '/User/projects/simdata/model-scaler.pickle'\n",
    "    :param key:         model name\n",
    "    \"\"\"\n",
    "\n",
    "    # this particular feature generator has no state\n",
    "    # or parameters\n",
    "    ffg = FeaturesGenerator()\n",
    "    \n",
    "    # scaler\n",
    "    ss = StandardScaler()\n",
    "    ss.__dict__ = joblib.load(f'{target_path}/scaler.pickle')\n",
    "    \n",
    "    # keras model\n",
    "    ksm = classifier_gen()\n",
    "    ksm.load_weights(f'{target_path}/weights.h5')\n",
    "    \n",
    "    pipe = make_pipeline(ffg, ss, ksm)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>\n",
    "## **training and validation**\n",
    "\n",
    "In this notebook demonstration we wrap the training and validation steps into the same method.  The data is split into train, validation, and test sets, with the latter being saved for further tests.\n",
    "\n",
    "**exercise / todos**\n",
    "\n",
    "To complete the demonstration, instead of hard-coding the `train_test_split` method, add a splitter class into the pipeline, like a cross-validator. \n",
    "\n",
    "The model encoder/decoder should also be input as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    context: MLClientCtx,\n",
    "    dataset: DataItem,\n",
    "    engineer_cls: str,\n",
    "    scaler_cls: str,\n",
    "    classifier_cls: str,\n",
    "    target_path: str,\n",
    "    key: str = '',\n",
    "    test_size: float = 0.1,\n",
    "    valid_size: float = 0.3,\n",
    "    batch_size: int = 1024,\n",
    "    epochs: int = 5,\n",
    "    verbose: bool = True,\n",
    "    random_state: int = 1,\n",
    "    ) -> None:\n",
    "    \"\"\"Train, validate, test and save a classifier model pipeline.\n",
    "    \n",
    "    Here we split the data, instantiate our pipeline and its models, and proceed\n",
    "    to training and validation.\n",
    "    \n",
    "    :param context:             function context\n",
    "    :param dataset:             cleaned input dataset\n",
    "    :param engineer_cls:        feature engineering class\n",
    "    :param scaler_cls:          scaler class\n",
    "    :param classifier_cls:      classifier class    \n",
    "    :param target_path:         destination for artifacts\n",
    "    :param key:                 model key in context\n",
    "    :param test_size:           (0.1) test set size as fraction\n",
    "    :param valid_size:          (0.3) validation set size as fraction\n",
    "    :param batch_size:          (1024) network feed batch size\n",
    "    :param epochs:              (5) training epochs\n",
    "    :param verbose:             (default True) Show metrics for \n",
    "                                training/validation steps\n",
    "        \n",
    "    Also included for demonstration are a randomly selected sample\n",
    "    of training parameters:\n",
    "    :param learning_rate: Step size at each iteration, constant.\n",
    "    \"\"\"\n",
    "    raw = get_context_table(dataset)\n",
    "\n",
    "    train, test = train_test_split(raw, test_size=test_size)\n",
    "    train, valid = train_test_split(train, test_size=valid_size)\n",
    "    \n",
    "    y_train = train.pop('labels')\n",
    "    y_valid = valid.pop('labels')\n",
    "    y_test = test.pop('labels')\n",
    "\n",
    "    # instantiate features engineer, scaler and classifier\n",
    "    Engineer = class_instance(engineer_cls)\n",
    "    Scaler = class_instance(scaler_cls)\n",
    "    classifier = class_instance(classifier_cls)\n",
    "\n",
    "    pipe = make_pipeline(Engineer(),\n",
    "                         Scaler(),\n",
    "                         classifier)\n",
    "    pipe.fit(train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(test)                          \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    context.log_result(\"accuracy\", float(acc))\n",
    "\n",
    "    my_pipeline_save(context, pipe, target_path, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _end of nuclio function definition_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"local-testing\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **testing your code locally**\n",
    "\n",
    "The function can be run locally and debugged/tested before deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import (mlconf,\n",
    "                   code_to_function,\n",
    "                   new_function,\n",
    "                   NewTask,\n",
    "                   new_model_server,\n",
    "                   mount_v3io)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set MLRun's DB path.  MLRun wil generate and store all of its tracking and metadata to the `MLRUN_DBATH` environment variable.  We have set a `TARGET_PATH` earlier in this notebook in the above section **[paths and parameters](#paths)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlconf.dbpath = 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = new_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_run = workflow.run(\n",
    "    name='data generator',\n",
    "    handler=data_generator,\n",
    "    params={\n",
    "        'samples': N_SAMPLES,\n",
    "        'features': M_FEATURES,\n",
    "        'neg_weight': CLASS_BALANCE, # this is a balanced dataset\n",
    "        'target_path': TARGET_PATH,\n",
    "        'key': 'simdata'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce verbosity on big data sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = workflow.run(\n",
    "    name='train, validate and store model',\n",
    "    handler=train,\n",
    "    inputs={\n",
    "        'dataset': datagen_run.output('features')},\n",
    "    params={\n",
    "        'scaler_cls':     'sklearn.preprocessing.data.StandardScaler',\n",
    "        'engineer_cls':   'functions.models.FeaturesEngineer',\n",
    "        'classifier_cls': 'functions.models.Classifier',\n",
    "        'target_path':     TARGET_PATH,\n",
    "        'key':             'model_dir',\n",
    "        'batch_size':      BATCH_SIZE,\n",
    "        'epochs':          10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"image\"></a>\n",
    "#### _Create a deployment image_\n",
    "\n",
    "Once debugged you can create a reusable image, and then deploy it for testing. In the following line we are converting the code block between the ```#nuclio: ignore``` and ```#nuclio: end-code``` to be run as a KubeJob.  Next we build an image named ```mlrun/mlrunlkeras:latest```.  _**It is important to ensure that this image has been `deploy`ed at least once, and that you have access to it.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfkeras_job = code_to_function(name='tfkeras_named_pipe',\n",
    "                               runtime=\"job\").apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfkeras_job.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfkeras_job.with_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_task = NewTask(out_path=TARGET_PATH).set_label('stage', 'dev')\n",
    "\n",
    "# datagen_task = NewTask(name='my-datagen', \n",
    "#                        handler='data_generator', \n",
    "#                        params= {\n",
    "#                            'samples': 10_000,\n",
    "#                            'features': 20,\n",
    "#                            'neg_weight': 0.5,\n",
    "#                            'target_path': TARGET_PATH,\n",
    "#                            'key': 'simdata'}, \n",
    "#                        base=base_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen_run = tfkeras_job.run(datagen_task, watch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create a KubeFlow Pipeline from our functions**\n",
    "\n",
    "Our pipeline will consist of two steps, ```load``` and ```train```.\n",
    "\n",
    "For complete details on KubeFlow Pipelines please refer to the following docs:\n",
    "1. **[KubeFlow pipelines](https://www.kubeflow.org/docs/pipelines/)**.\n",
    "2. **[kfp.dsl Python package](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html#module-kfp.dsl)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, the model server file name in the ```new_model_server``` function call below should identical in every respect to the name of the model server notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srvfn = new_model_server(\"tfkeras_serving_v2\",  \n",
    "                         model_class=\"MyKerasClassifier\",   \n",
    "                         filename=\"model_server.ipynb\")\n",
    "srvfn.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Sklearn and KubeFlow\",\n",
    "    description=\"Shows how to use mlrun/kfp.\"\n",
    ")\n",
    "def tfkeras_pipeline(\n",
    "    neg_weight = [0.5, 0.1],\n",
    "):\n",
    "\n",
    "    datagen = tfkeras_job.as_step(\n",
    "        name='data generator',\n",
    "        handler='data_generator',\n",
    "        out_path=TARGET_PATH, \n",
    "        params={        \n",
    "            'samples':         N_SAMPLES,\n",
    "            'features':        M_FEATURES,\n",
    "            'neg_weight':      CLASS_BALANCE,\n",
    "            'target_path':     TARGET_PATH,\n",
    "            'key':            'features'},\n",
    "        outputs=['features']).apply(mount_v3io())\n",
    "    \n",
    "    train = tfkeras_job.as_step(\n",
    "        name='sklearn pipe train',\n",
    "        handler='train',\n",
    "        out_path=TARGET_PATH, \n",
    "        inputs={'dataset': datagen.outputs['features']},\n",
    "        outputs=['model_dir'],\n",
    "        params={\n",
    "            'scaler_cls':     'sklearn.preprocessing.data.StandardScaler',\n",
    "            'engineer_cls':   'functions.models.FeaturesEngineer',\n",
    "            'classifier_cls': 'functions.models.Classifier',\n",
    "            'target_path':     TARGET_PATH,\n",
    "            'key':             'model_dir',\n",
    "            'batch_size':      BATCH_SIZE,\n",
    "            'epochs':          10}).apply(mount_v3io())\n",
    "\n",
    "    # define a nuclio-serving function, generated from a notebook file\n",
    "    srvfn.deploy_step(project=\"refactoring-demos\", \n",
    "                      models={\"tfkeras_joblib_v2\": train.outputs[\"model_dir\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compile the pipeline\"></a>\n",
    "### _compile the pipeline_\n",
    "\n",
    "We can compile our KubeFlow pipeline and produce a yaml description of the pipeline worflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(TARGET_PATH, exist_ok=True)\n",
    "kfp.compiler.Compiler().compile(tfkeras_pipeline, TARGET_PATH+\"/mlrunpipe.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(namespace=\"default-tenant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following line will run the pipeline as a job::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    'neg_weight' : [0.5, 0.1]    \n",
    "}\n",
    "\n",
    "run_result = client.create_run_from_pipeline_func(\n",
    "    tfkeras_pipeline, \n",
    "    arguments, \n",
    "    run_name=\"tfkeras_latest_v2\",\n",
    "    experiment_name=\"tfkeras-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlrun clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
