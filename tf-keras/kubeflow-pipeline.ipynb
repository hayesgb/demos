{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tensorflow-Keras and Scikit-Learn With MLRun**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLRun is an open-source Python package that provides a framework for running machine learning tasks transparently in multiple, scalable, runtime environments.  MLRun provides tracking of code, metadata, inputs, outputs and the results of machine learning pipelines. \n",
    "\n",
    "In this notebook we\"ll compose a pipeline that deploys a classifier model, and uses it as the input in a training and validation step. We'll be working with a synthetic features matrix of dimension 10 million rows by 20 features and a binary label.  The model will be a 2-layer neural net classifier using **[tensorflow-keras](https://www.tensorflow.org/)** (v2.0.0b1), without gpu support.\n",
    "\n",
    "The dataset we create is balanced, however there is a `weight` parameter in the data generator function specifying the fraction of observations that are labeled 0/False. The number of samples and features are also parameters.  The demonstration could be modified easily to allow for a more fine-grained control over the simulated dataset either by adding more parameters or replacing the underlying function altogether.\n",
    "\n",
    "The training and validation step employs a scikit learn `Pipeline` to perform feature engineering. Some of the feature engineering needs to be done _**after**_ the train-valid-test set split. In some preprocessing scenarios we might estimate a data transformation on the training set before model training, and then apply the estimate to the validation and test sets before prediction. Since we need to perform the same transformation pre-inference, all pipeline model steps are stored.\n",
    "\n",
    "Serializing models can be challenging for number of reasons:  a pipeline with multiple steps may require just as many encoding and decoding routines--applying joblib to a Keras model that has been wrapped in a scikit-learn api fails.  Since we have the model architecture in a class definition, all we need to do is save the weights.  Some steps in a pipeline may have no internal state to store, while others can be stored and loaded using `joblib`.  Most of it all boils down to storing dicts/json with numpy objects.\n",
    "\n",
    "One of the upsides of the present architecture is that we can mix many simulations of data with a given model estimator, or many models with a given data sample and track everything in **MLRun**.  Research, development, and deployment, all on one page, running under multiple configurations, limited only by the compute resources at our disposal.\n",
    "\n",
    "\n",
    "#### **notebook take-aways**\n",
    "* write and test reusable and replaceable **[MLRun](https://github.com/mlrun)** components in a notebook\n",
    "* store and load models\n",
    "* run the components as a **[KubeFlow](https://www.kubeflow.org/)** pipeline\n",
    "\n",
    "<a id='top'></a>\n",
    "#### **steps**\n",
    "**[install the python mlrun package](#install)**<br>\n",
    "**[nuclio code section](#nuclio-code-section)**<br>\n",
    "    - [nuclio's ignore](#ignore)<br>\n",
    "    - [function dependencies](#function-dependencies)<br>\n",
    "\n",
    "**[components](#components)**<br>\n",
    "    - [supporting functions](#utilties)<br>\n",
    "    - [data simulation](#datasim)<br>\n",
    "    - [feature engineering](#feateng)<br>\n",
    "    - [a classifier](#classifier)<br>\n",
    "    - [training and validation](#train)<br>\n",
    "**[local tests](#local-testing)**<br>\n",
    "**[compose pipeline](#image)**<br>\n",
    "**[run](#run)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **notebook installs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will reinstall the latest development version of ```mlrun```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip uninstall mlrun -y\n",
    "        \n",
    "    !pip install -U git+https://github.com/mlrun/mlrun.git@development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the KubeFlow pipelines package ```kfp```. For more information see the **[KubeFlow documentation on nuclio](https://www.kubeflow.org/docs/components/misc/nuclio/)** and  **[Kubeflow pipelines and nuclio](https://github.com/kubeflow/pipelines/tree/master/components/nuclio)**. For logging the estimated machine learning models we'll use ```joblib```'s ```dump``` and ```load```. For more details see **[Joblib: running Python functions as pipeline jobs](https://joblib.readthedocs.io/en/latest/index.html)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install -U kfp joblib seaborn tensorflow==2.0.0b1 keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nuclio-code-section\"><a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **nuclio code section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ignore'></a>\n",
    "### _nuclio's **ignore** notation_\n",
    "\n",
    "You\"ll write all the code that gets packaged for execution between the tags ```# nuclio: ignore```, meaning ignore all the code here and above, and ```# nuclio: end-code```, meaning ignore everything after this annotation.  The **[docs](https://github.com/nuclio/nuclio-jupyter#creating-and-debugging-functions-using-nuclio-magic)** also suggest another approach: we can use ```# nuclio: start``` at the first relevant code cell instead of marking all the cells above with ```# nuclio: ignore```.\n",
    "\n",
    "See the **[nuclio-jupyter](https://github.com/nuclio/nuclio-jupyter)** repo for further information on these and many other **[nuclio magic commands](https://github.com/nuclio/nuclio-jupyter#creating-and-debugging-functions-using-nuclio-magic)** that make it easy to transform a Jupyter notebook environment into a platform for developing production-quality, machine learning systems.\n",
    "\n",
    "The ```nuclio-jupyter``` package provides methods for automatically generating and deploying nuclio serverless functions from code, repositories or Jupyter notebooks. **_If you have never run nuclio functions in your notebooks, please uncomment and run the following_**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        !pip install nuclio-jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two lines _**should be in the same cell**_ and mark the start of your mchine learning coding section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"function-dependencies\"></a>\n",
    "### _function dependencies_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installs made in the section **[Setup](#Setup)** covered the Jupyter environment within which this notebook runs.  However, we need to ensure that all the dependencies our nuclio function relies upon (such as ```matplotlib```, ```sklearn```, ```lightgbm```), will be available when that code is wrapped up into a nuclio function _**on some presently unknown runtime**_.   Within the nuclio code section we can ensure these dependencies get built into the function with the ```%nuclio cmd``` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio cmd -c pip install -U matplotlib tensorflow==2.0.0b1 keras sklearn pandas numpy joblib\n",
    "%nuclio cmd -c pip install -U git+https://github.com/mlrun/mlrun.git@development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\"ll use a standard base image here, however the build step can be shortened by preparing images with pre-installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.build.baseImage to 'python:3.6-jessie'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config spec.build.baseImage = \"python:3.6-jessie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _imports_\n",
    "\n",
    "Some of the functionality is provided in supporting components within the ```functions``` folder.<br>\n",
    "\n",
    "Links to the code:\n",
    "- **[datasets](functions/datasets.py)**:&emsp;generate simulation data\n",
    "- **[files](functions/file_fs.py)**:&emsp;&emsp;&emsp;save and load files\n",
    "- **[models](function/model_fs.py)**:&nbsp; &emsp;save, load, and instantiate models\n",
    "- **[plots](functions/plot_fs.py)**:&emsp;  &emsp; sundry plotting functions\n",
    "- **[tables](functions/tables.py)**:&emsp; &nbsp; &nbsp;logging and retrieving table artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/User/.pythonlibs/lib/python3.6/site-packages/sqlalchemy/ext/declarative/clsregistry.py:129: SAWarning: This declarative base already contains a class with the same class name and module name as mlrun.db.sqldb.Label, and will be replaced in the string-lookup table.\n",
      "  % (item.__module__, item.__name__)\n"
     ]
    }
   ],
   "source": [
    "from functions.datasets import create_binary_classification\n",
    "from functions.tables import log_context_table, get_context_table\n",
    "from functions.models import FeaturesEngineer, Classifier, class_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Union, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='paths'></a>\n",
    "### _paths and parameters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PATH = '/User/mlrun/simdata'\n",
    "IMAGE_NAME = \"mlrun/mlruntfkeras:latest\"\n",
    "MODEL_NAME = 'my-binclass-tfkeras-model'\n",
    "\n",
    "# data simulation and ml training parameter\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS= 3\n",
    "N_SAMPLES = 1_000_000\n",
    "M_FEATURES = 20\n",
    "CLASS_BALANCE = 0.5\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"components\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='delete'></a>\n",
    "## **data generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "    context: MLClientCtx,\n",
    "    samples: int,\n",
    "    features: int,\n",
    "    features_hdr: Optional[List[str]],\n",
    "    neg_weight: float,\n",
    "    target_path: str,\n",
    "    key: str\n",
    ") -> None:\n",
    "    \"\"\"Generate raw data for this pipeline\n",
    "    \n",
    "    This component will be the entry point of the pipeline.\n",
    "    \n",
    "    In this demonstration our component is a simple wrapper for scikit learn's \n",
    "    `make_classification`, a convenient utility enabling us to build\n",
    "    and test a pipeline from start to finish with a clean and \n",
    "    predictable dataset. By fiddling with neg_weight, we can also take a \n",
    "    quick look at the effect of class balance on our model before exposing it\n",
    "    to the kind of data we find in the real world.\n",
    "    \n",
    "    :param context:       function context\n",
    "    :param samples:       number of samples (rows) to generate\n",
    "    :param features:      number of features (cols)\n",
    "    :param features_hdr:  (optional) header for the features array\n",
    "    :param neg_weights:   fraction of negative samples\n",
    "    :param target_path:   destination for data including file name\n",
    "    :param key:           context key of data\n",
    "    \"\"\"\n",
    "    if features_hdr:\n",
    "        assert len(features_hdr)==m_features, f\"features header dimension mismatch for {name}\"\n",
    "    data = create_binary_classification(\n",
    "                context, n_samples=samples, m_features=features,\n",
    "                features_hdr=features_hdr,  weight=neg_weight, \n",
    "                target_path=target_path, key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feateng'></a>\n",
    "## **feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For code please see the custom sklearn transformer `FeaturesEngineer` in **[models.py](functions/models.py)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classifier\"></a>\n",
    "## **classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For code please see `KerasClassifier` and `classifier_gen` in **[models.py](functions/models.py)**.  This method generates a small keras Sequential model with 2 layers which gets wrapped in a `KerasClassifier` class. The latter provides it with a convenient sklearn interface for use in **[sklearn Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn-pipeline-pipeline)**. The list of metrics collected during training can also be found in the same module as `METRICS` and includes accuracy, precision, recall, auc and a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-save'></a>\n",
    "## **save model**\n",
    "\n",
    "The model presented here has three stages, wrapped into an sklearn pipeline.  In order to save this model pipeline, each of its components may have to be saved independently.  There may be other advantages to serializing the pipeline components separately.\n",
    "\n",
    "In our pipeline, the `FeaturesEngineer` has no state so we just create a new one during load. \n",
    "\n",
    "The `StandardScaler`'s estimates need to be re-used when transforming new data, so it is\n",
    "pickled using `joblib` (it contains arrays).  Its filename is `{path.join(target_path,name)}-scaler.pickle`. **It is important to remember that feature stores need more than feature arrays, they need code and often data in the form of parameter estimates, correlation matrices and so on so that these may be used again on new data, and for forensics.**\n",
    "\n",
    "Keras models have an architecture saved as json, and corresponding weights are saved in hdf5 format. The architecture can be recreated by instantiating the class or converting the json representation into a model, with the weights loaded into that structure. Filenames for the model are `{path.join(target_path,name)}-weights.h5` and `{path.join(target_path,name)}-model.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pipeline_save(\n",
    "    context: MLClientCtx,\n",
    "    pipe: Pipeline,\n",
    "    target_path: str,\n",
    "    key: str\n",
    "):\n",
    "    \"\"\"Serialize a specific pipeline.\n",
    "    \n",
    "    :param context:         function context\n",
    "    :param pipe:            estimated model pipeline\n",
    "    :param target_path:     destination for saved model\n",
    "    :param name:            model name\n",
    "    \"\"\"\n",
    "    if target_path:\n",
    "        os.makedirs(target_path, exist_ok=True)\n",
    "    \n",
    "    # StandardScaler\n",
    "    joblib.dump(\n",
    "        pipe.steps[1][1].__dict__, \n",
    "        target_path + '/scaler.pickle')\n",
    "    \n",
    "    # Keras model--if we have the class code, we don't need this\n",
    "    json.dump(\n",
    "        pipe.steps[2][1].model.to_json(), \n",
    "        open(f'{target_path}/model.json', 'w'))\n",
    "    pipe.steps[2][1].model.save_weights(f'{target_path}/weights.h5')\n",
    "    \n",
    "    context.log_artifact('model', \n",
    "                         target_path=target_path, \n",
    "                         labels={\"engineer\": \"functions.models.FeaturesEngineer\",\n",
    "                                 \"scaler\": \"sklearn.preprocessing.StandardScaler\",\n",
    "                                 'model': 'tensorflow.keras.sklearn.KerasClassifier',\n",
    "                                 \"type\": \"classifier\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pipeline_load(\n",
    "    target_path: str,\n",
    "    key: str\n",
    ") -> Pipeline:\n",
    "    \"\"\"Deserialize a specific pipeline.\n",
    "    \n",
    "    See 'pipeline_save' for details.\n",
    "    \n",
    "    :param target_path: location of saved model + prefix of file names\n",
    "                        For example '/User/projects/simdata/model-scaler.pickle'\n",
    "    :param key:         model name\n",
    "    \"\"\"\n",
    "\n",
    "    # this particular feature generator has no state\n",
    "    # or parameters\n",
    "    ffg = FeaturesGenerator()\n",
    "    \n",
    "    # scaler\n",
    "    ss = StandardScaler()\n",
    "    ss.__dict__ = joblib.load(f'{target_path}/scaler.pickle')\n",
    "    \n",
    "    # keras model\n",
    "    ksm = classifier_gen()\n",
    "    ksm.load_weights(f'{target_path}/weights.h5')\n",
    "    \n",
    "    pipe = make_pipeline(ffg, ss, ksm)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>\n",
    "## **training and validation**\n",
    "\n",
    "In this notebook demonstration we wrap the training and validation steps into the same method.  The data is split into train, validation, and test sets, with the latter being saved for further tests.\n",
    "\n",
    "**exercise / todos**\n",
    "\n",
    "To complete the demonstration, instead of hard-coding the `train_test_split` method, add a splitter class into the pipeline, like a cross-validator. \n",
    "\n",
    "The model encoder/decoder should also be input as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    context: MLClientCtx,\n",
    "    dataset: DataItem,\n",
    "    engineer_cls: str,\n",
    "    scaler_cls: str,\n",
    "    classifier_cls: str,\n",
    "    target_path: str,\n",
    "    key: str = '',\n",
    "    test_size: float = 0.1,\n",
    "    valid_size: float = 0.3,\n",
    "    batch_size: int = 1024,\n",
    "    epochs: int = 5,\n",
    "    verbose: bool = True,\n",
    "    random_state: int = 1,\n",
    "    ) -> None:\n",
    "    \"\"\"Train, validate, test and save a classifier model pipeline.\n",
    "    \n",
    "    Here we split the data, instantiate our pipeline and its models, and proceed\n",
    "    to training and validation.\n",
    "    \n",
    "    :param context:             function context\n",
    "    :param dataset:             cleaned input dataset\n",
    "    :param engineer_cls:        feature engineering class\n",
    "    :param scaler_cls:          scaler class\n",
    "    :param classifier_cls:      classifier class    \n",
    "    :param target_path:         destination for artifacts\n",
    "    :param key:                 model key in context\n",
    "    :param test_size:           (0.1) test set size as fraction\n",
    "    :param valid_size:          (0.3) validation set size as fraction\n",
    "    :param batch_size:          (1024) network feed batch size\n",
    "    :param epochs:              (5) training epochs\n",
    "    :param verbose:             (default True) Show metrics for \n",
    "                                training/validation steps\n",
    "        \n",
    "    Also included for demonstration are a randomly selected sample\n",
    "    of training parameters:\n",
    "    :param learning_rate: Step size at each iteration, constant.\n",
    "    \"\"\"\n",
    "    raw = get_context_table(dataset)\n",
    "\n",
    "    train, test = train_test_split(raw, test_size=test_size)\n",
    "    train, valid = train_test_split(train, test_size=valid_size)\n",
    "    \n",
    "    y_train = train.pop('labels')\n",
    "    y_valid = valid.pop('labels')\n",
    "    y_test = test.pop('labels')\n",
    "\n",
    "    # instantiate features engineer, scaler and classifier\n",
    "    Engineer = class_instance(engineer_cls)\n",
    "    Scaler = class_instance(scaler_cls)\n",
    "    classifier = class_instance(classifier_cls)\n",
    "\n",
    "    # we want to save the features, and perhaps use them elsewhere\n",
    "    # so so the Engineer gets the header (deprecate?), path and key:\n",
    "    engineer = Engineer(raw.columns, target_path, 'features')\n",
    "    # the other components have no parameters in this demonstration:\n",
    "\n",
    "    pipe = make_pipeline(engineer,\n",
    "                         Scaler(),\n",
    "                         classifier)\n",
    "    pipe.fit(train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(test)                          \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    context.log_result(\"accuracy\", float(acc))\n",
    "\n",
    "    my_pipeline_save(context, pipe, target_path, 'tf-keras-pipe-key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _end of nuclio function definition_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"local-testing\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **testing your code locally**\n",
    "\n",
    "The function can be run locally and debugged/tested before deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import (mlconf,\n",
    "                   code_to_function,\n",
    "                   new_function,  \n",
    "                   new_model_server,\n",
    "                   mount_v3io)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set MLRun's DB path.  MLRun wil generate and store all of its tracking and metadata to the `MLRUN_DBATH` environment variable.  We have set a `TARGET_PATH` earlier in this notebook in the above section **[paths and parameters](#paths)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlconf.dbpath = \"http://mlrun-api:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = new_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing /User/mlrun/simdata/simdata-1e06X20.parquet\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> \n",
       ".dictlist {\n",
       "  background-color: #b3edff; \n",
       "  text-align: center; \n",
       "  margin: 4px; \n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer; \n",
       "  background-color: #ffe6cc; \n",
       "  text-align: left; \n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "  \n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "  \n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }  \n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "  \n",
       "  \n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td><div title=\"d5e893bf80ad4cd681a87178c21e4642\">...1e4642</div></td>\n",
       "      <td>0</td>\n",
       "      <td>Dec 26 00:45:16</td>\n",
       "      <td>completed</td>\n",
       "      <td>data generator</td>\n",
       "      <td><div class=\"dictlist\">host=jupyter-qlqrqnzi25-vogv2-79db4f79d-qt2wf</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">samples=1000000</div><div class=\"dictlist\">features=20</div><div class=\"dictlist\">neg_weight=0.5</div><div class=\"dictlist\">target_path=/User/mlrun/simdata</div><div class=\"dictlist\">key=simdata</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"/User/mlrun/simdata/simdata-1e06X20.parquet\">features</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result94a3f16d-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result94a3f16d-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result94a3f16d\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result94a3f16d-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run d5e893bf80ad4cd681a87178c21e4642  , !mlrun logs d5e893bf80ad4cd681a87178c21e4642 \n",
      "[mlrun] 2019-12-26 00:45:20,753 run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "datagen_run = workflow.run(\n",
    "    name='data generator',\n",
    "    handler=data_generator,\n",
    "    params={\n",
    "        'samples': N_SAMPLES,\n",
    "        'features': M_FEATURES,\n",
    "        'neg_weight': CLASS_BALANCE, # this is a balanced dataset\n",
    "        'target_path': TARGET_PATH,\n",
    "        'key': 'simdata'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /User/.pythonlibs/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "630000 20\n",
      "Train on 630000 samples\n",
      "630000/630000 [==============================] - 24s 38us/sample - loss: 0.1655 - tp: 299095.0000 - fp: 21876.0000 - tn: 293245.0000 - fn: 15784.0000 - accuracy: 0.9402 - precision: 0.9318 - recall: 0.9499 - auc: 0.9850\n",
      "100000 20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style> \n",
       ".dictlist {\n",
       "  background-color: #b3edff; \n",
       "  text-align: center; \n",
       "  margin: 4px; \n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer; \n",
       "  background-color: #ffe6cc; \n",
       "  text-align: left; \n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "  \n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "  \n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }  \n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "  \n",
       "  \n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td><div title=\"3c0d63660cfc4b6481bb26ce28fa5cb5\">...fa5cb5</div></td>\n",
       "      <td>0</td>\n",
       "      <td>Dec 26 00:45:20</td>\n",
       "      <td>completed</td>\n",
       "      <td>train, validate and store model</td>\n",
       "      <td><div class=\"dictlist\">host=jupyter-qlqrqnzi25-vogv2-79db4f79d-qt2wf</div></td>\n",
       "      <td><div title=\"/User/mlrun/simdata/simdata-1e06X20.parquet\">dataset</div></td>\n",
       "      <td><div class=\"dictlist\">scaler_cls=sklearn.preprocessing.data.StandardScaler</div><div class=\"dictlist\">engineer_cls=functions.models.FeaturesEngineer</div><div class=\"dictlist\">classifier_cls=functions.models.Classifier</div><div class=\"dictlist\">target_path=/User/mlrun/simdata</div><div class=\"dictlist\">key=model</div><div class=\"dictlist\">batch_size=1024</div><div class=\"dictlist\">epochs=10</div></td>\n",
       "      <td><div class=\"dictlist\">accuracy=0.98014</div></td>\n",
       "      <td><div title=\"/User/mlrun/simdata\">model</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result945c33cb-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result945c33cb-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result945c33cb\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result945c33cb-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run 3c0d63660cfc4b6481bb26ce28fa5cb5  , !mlrun logs 3c0d63660cfc4b6481bb26ce28fa5cb5 \n",
      "[mlrun] 2019-12-26 00:45:48,653 run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "train_run = workflow.run(\n",
    "    name='train, validate and store model',\n",
    "    handler=train,\n",
    "    inputs={\n",
    "        'dataset': datagen_run.output('features')},\n",
    "    params={\n",
    "        'scaler_cls':     'sklearn.preprocessing.data.StandardScaler',\n",
    "        'engineer_cls':   'functions.models.FeaturesEngineer',\n",
    "        'classifier_cls': 'functions.models.Classifier',\n",
    "        'target_path':     TARGET_PATH,\n",
    "        'key':             'model',\n",
    "        'batch_size':      BATCH_SIZE,\n",
    "        'epochs':          10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"image\"></a>\n",
    "#### _Create a deployment image_\n",
    "\n",
    "Once debugged you can create a reusable image, and then deploy it for testing. In the following line we are converting the code block between the ```#nuclio: ignore``` and ```#nuclio: end-code``` to be run as a KubeJob.  Next we build an image named ```mlrun/mlrunlkeras:latest```.  _**It is important to ensure that this image has been built at least once, and that you have access to it.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfkeras_job = code_to_function(name='tfkeras-named-pipe', runtime=\"job\").apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create a KubeFlow Pipeline from our functions**\n",
    "\n",
    "Our pipeline will consist of two instead of three steps, ```load``` and ```train```.  We\"ll drop the ```test```\n",
    "here since at the end of this deployment we can test the system with API requests.\n",
    "\n",
    "For complete details on KubeFlow Pipelines please refer to the following docs:\n",
    "1. **[KubeFlow pipelines](https://www.kubeflow.org/docs/pipelines/)**.\n",
    "2. **[kfp.dsl Python package](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html#module-kfp.dsl)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, the model server file name in the ```new_model_server``` function call below should identical in every respect to the name of the model server notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Sklearn and KubeFlow\",\n",
    "    description=\"Shows how to use mlrun/kfp.\"\n",
    ")\n",
    "def tfkeras_pipeline(\n",
    "):\n",
    "\n",
    "    datagen = tfkeras_job.as_step(\n",
    "        handler='data_generator',\n",
    "        out_path=TARGET_PATH, \n",
    "        params={        \n",
    "            'samples':         N_SAMPLES,\n",
    "            'features':        M_FEATURES,\n",
    "            'neg_weight':      CLASS_BALANCE,\n",
    "            'target_path':     TARGET_PATH,\n",
    "            'key':            'simdata'},\n",
    "        outputs=['features'])\n",
    "    \n",
    "    train = tfkeras_job.as_step(\n",
    "        handler='train',\n",
    "        out_path=TARGET_PATH, \n",
    "        inputs={'dataset': datagen.outputs['features']},\n",
    "        outputs=['model'],\n",
    "        params={\n",
    "            'scaler_cls':     'sklearn.preprocessing.data.StandardScaler',\n",
    "            'engineer_cls':   'functions.models.FeaturesEngineer',\n",
    "            'classifier_cls': 'functions.models.Classifier',\n",
    "            'target_path':     TARGET_PATH,\n",
    "            'key':             'model',\n",
    "            'batch_size':      BATCH_SIZE,\n",
    "            'epochs':          10})        \n",
    "\n",
    "    # define a nuclio-serving function, generated from a notebook file\n",
    "    srvfn = new_model_server(\n",
    "        \"tfkeras-serving\", \n",
    "        model_class=\"TFKerasClassifier\", \n",
    "        filename=\"model-server.ipynb\")\n",
    "    \n",
    "    # deploy the model serving function with inputs from the training stage\n",
    "    deploy = srvfn.with_v3io(\"User\", \"~/\")\n",
    "    deploy = deploy.deploy_step(project=\"refactor-demos\", \n",
    "                                models={\"tfkeras_joblib\": train.outputs[\"model\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compile the pipeline\"></a>\n",
    "### _compile the pipeline_\n",
    "\n",
    "We can compile our KubeFlow pipeline and produce a yaml description of the pipeline worflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TARGET_PATH, exist_ok=True)\n",
    "kfp.compiler.Compiler().compile(tfkeras_pipeline, TARGET_PATH+\"/mlrunpipe.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(namespace=\"default-tenant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following line will run the pipeline as a job::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "}\n",
    "\n",
    "run_result = client.create_run_from_pipeline_func(\n",
    "    tfkeras_pipeline, \n",
    "    arguments, \n",
    "    run_name=\"tfkeras_latest\",\n",
    "    experiment_name=\"tfkeras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/User/.pythonlibs/lib/python3.6/site-packages/sqlalchemy/ext/declarative/clsregistry.py:129: SAWarning: This declarative base already contains a class with the same class name and module name as mlrun.db.sqldb.Label, and will be replaced in the string-lookup table.\n",
      "  % (item.__module__, item.__name__)\n",
      "[mlrun] 2019-12-26 00:52:02,087 using in-cluster config.\n",
      "state      started          type     name\n",
      "Failed     Dec 26 00:31:16  job      kubeflow-pipeline-67222\n",
      "Succeeded  Dec 26 00:14:40  build    mlrun-build-kubeflow-pipeline-gft9m\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "!mlrun clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
