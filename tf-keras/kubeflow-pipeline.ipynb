{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Keras and Scikit-Learn With MLRun: Kaggle's Credit Fraud Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLRun is an open-source Python package that provides a framework for running machine learning tasks transparently in multiple, scalable, runtime environments.  MLRun provides tracking of code, metadata, inputs, outputs and the results of machine learning pipelines. \n",
    "\n",
    "In this notebook we\"ll compose a pipeline that deploys a classifier model, and uses it as the input in either evaluation, inference, or retrain steps. We'll be working with **[Kaggle's Synthetic Financial Datasets For Fraud Detection](https://www.kaggle.com/ntnu-testimon/paysim1)**, a synthetic mobile transactions dataset with 6362620 rows, 10 features and a binary label.  The model will be a dense neural net classifier written with **[keras](https://keras.io/)** (v2.3.1), using a **[tensorflow](https://www.tensorflow.org/)** backend (v1.14.0), and without gpu support.\n",
    "\n",
    "One of the challeges with the paysim dataset is it's imbalance. For more details on some of the issues, take a look at **[Classification on imbalanced data](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)** covering the same dataset. We'll borrow some of the feature transformations presented there as part of a pipeline within a pipeline.\n",
    "\n",
    "Another transformation we'll apply is a standard normalization of our feature input matrix.  This ```fit``` is performed on the training data only, and this estimated transformer is saved and reused on any data going into the model--during validation, testing, and inference the data must be transformed using this pre-estimate. One potential source of model drift would be the case where this pre-estimated normalizer fails.  \n",
    "\n",
    "All of the fitted transformations and latest estimated model are run as an sklearn pipeline, then saved and deployed for later testing and inference. \n",
    "\n",
    "#### **notebook take-aways**\n",
    "* write and test reusable **[MLRun](https://github.com/mlrun)** components in a notebook\n",
    "* run the components as a **[KubeFlow](https://www.kubeflow.org/)** pipeline\n",
    "\n",
    "<a id='top'></a>\n",
    "#### **steps**\n",
    "**[install the python mlrun package](#install)**<br>\n",
    "**[nuclio code section](#nuclio-code-section)**<br>\n",
    "    - [nuclio's ignore](#ignore)<br>\n",
    "    - [function dependencies](#function-dependencies)<br>\n",
    "\n",
    "**[components](#components)**<br>\n",
    "    - [supporting functions](#utilties)<br>\n",
    "    - [feature engineering](#feateng)<br>\n",
    "    - [a classifier](#classifier)<br>\n",
    "    - [training and validation](#train)<br>\n",
    "**[local tests](#local-testing)**<br>\n",
    "**[compose pipeline](#image)**<br>\n",
    "**[run](#run)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **notebook installs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will reinstall the latest development version of ```mlrun```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install -U git+https://github.com/mlrun/mlrun.git@development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the KubeFlow pipelines package ```kfp```. For more information see the **[KubeFlow documentation on nuclio](https://www.kubeflow.org/docs/components/misc/nuclio/)** and  **[Kubeflow pipelines and nuclio](https://github.com/kubeflow/pipelines/tree/master/components/nuclio)**. For logging the estimated machine learning models we\"ll use ```joblib```\"s [```dump``` and ```load```](https://joblib.readthedocs.io/en/latest/persistence.html#persistence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install -U kfp joblib seaborn tensorflow==1.14 keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nuclio-code-section\"><a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **nuclio code section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ignore'></a>\n",
    "### _nuclio\"s **ignore** notation_\n",
    "\n",
    "You\"ll write all the code that gets packaged for execution between the tags ```# nuclio: ignore```, meaning ignore all the code here and above, and ```# nuclio: end-code```, meaning ignore everything after this annotation.  Methods in this code section can be called separately if designed as such (```acquire```, ```split```, ```train```, ```test```), or as you\"ll discover below, they are most often \"chained\" together to form a pipeline where the output of one stage serves as the input to the next. The **[docs](https://github.com/nuclio/nuclio-jupyter#creating-and-debugging-functions-using-nuclio-magic)** also suggest another approach: we can use ```# nuclio: start``` at the first relevant code cell instead of marking all the cells above with ```# nuclio: ignore```.\n",
    "\n",
    "See the **[nuclio-jupyter](https://github.com/nuclio/nuclio-jupyter)** repo for further information on these and many other **[nuclio magic commands](https://github.com/nuclio/nuclio-jupyter#creating-and-debugging-functions-using-nuclio-magic)** that make it easy to transform a Jupyter notebook environment into a platform for developing production-quality, machine learning systems.\n",
    "\n",
    "The ```nuclio-jupyter``` package provides methods for automatically generating and deploying nuclio serverless functions from code, repositories or Jupyter notebooks. **_If you have never run nuclio functions in your notebooks, please uncomment and run the following_**: ```!pip install nuclio-jupyter```\n",
    "\n",
    "The following two lines _**should be in the same cell**_ and mark the start of your mchine learning coding section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"function-dependencies\"></a>\n",
    "### _function dependencies_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installs made in the section **[Setup](#Setup)** covered the Jupyter environment within which this notebook runs.  However, we need to ensure that all the dependencies our nuclio function relies upon (such as ```matplotlib```, ```sklearn```, ```lightgbm```), will be available when that code is wrapped up into a nuclio function _**on some presently unknown runtime**_.   Within the nuclio code section we can ensure these dependencies get built into the function with the ```%nuclio cmd``` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio cmd -c pip install -U matplotlib tensorflow==1.14.0 keras sklearn pandas numpy joblib\n",
    "%nuclio cmd -c pip install -U git+https://github.com/mlrun/mlrun.git@7604c4d35e076897b49815f8f1cb8ee13cbd0286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\"ll use a standard base image here, however the build step can be shortened by preparing images with pre-installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio config spec.build.baseImage = \"python:3.6-jessie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _imports_\n",
    "\n",
    "Some of the functionality is provided in supporting components within the functions folder:\n",
    "- **[files](functions/file_functions.py)**\n",
    "- **[models](function/model_functions.py)**\n",
    "- **[plots](functions/plot_functions.py)**\n",
    "- **[tables](functions/table_functions.py)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='paths'></a>\n",
    "### _source and destination paths_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PATH = \"/User/projects/paysim\"\n",
    "SRC_PATH = TARGET_PATH\n",
    "SRC_URL = ''\n",
    "SRCNAME = \"data/PS_20174392719_1491204439457_log.csv.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"components\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feateng'></a>\n",
    "## **feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudFeaturesGenerator(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Generate features from raw input.\n",
    "\n",
    "    A standard transformer mixin that can be inserted into a scikit learn Pipeline.\n",
    "    \n",
    "    These modifications to the features matrix follows the approach taken by \n",
    "    https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "    where this data set is looked at in some detail.\n",
    "\n",
    "    To use, \n",
    "    >>> ffg = FraudFeaturesGenerator()\n",
    "    >>> ffg.fit(X)\n",
    "    >>> x_transformed = ffg.transform(X)\n",
    "    or\n",
    "    >>> ffg = FraudFeaturesGenerator()\n",
    "    >>> x_transformed = ffg.fit_transform(X)\n",
    "    \n",
    "    In a pipeline:\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> from sklearn.preprocessing import StandardScaler\n",
    "    >>> transformers = [('feature_gen', FraudFeaturesGeneratorFeature()), \n",
    "                        ('scaler', StandardScaler())]\n",
    "    >>> transformer_pipe = Pipeline(transformers)\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        \"\"\"fit is unused here\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(\n",
    "        self,\n",
    "        x: pd.DataFrame,\n",
    "        y=None\n",
    "    ): -> pd.DataFrame:\n",
    "        \"\"\"Transform raw input features a preprocessing step.\n",
    "        \n",
    "        :param x: Raw input features, as a pandas Dataframe \n",
    "        \n",
    "        Returns a cleaned DataFrame of features.\n",
    "        \"\"\"\n",
    "        x.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1, inplace=True)\n",
    "        x = x.loc[(X.type == 'TRANSFER') | (x.type == 'CASH_OUT')]\n",
    "        \n",
    "        # Binary-encoding of labelled data in 'type'\n",
    "        x.loc[x.type == 'TRANSFER', 'type'] = 0\n",
    "        x.loc[x.type == 'CASH_OUT', 'type'] = 1\n",
    "        x.type = x.type.astype(int) # convert dtype('O') to dtype(int)\n",
    "        \n",
    "        # cannot happen so mark\n",
    "        x.loc[(x.oldbalanceDest == 0) & (x.newbalanceDest == 0) & (x.amount != 0), \\\n",
    "              ['oldbalanceDest', 'newbalanceDest']] = - 1\n",
    "        x.loc[(x.oldbalanceOrg == 0) & (x.newbalanceOrig == 0) & (x.amount != 0), \\\n",
    "              ['oldbalanceOrg', 'newbalanceOrig']] = np.nan\n",
    "        \n",
    "        # create 2 new features (columns) recording errors in the originating and \n",
    "        # destination accounts for each transaction. \n",
    "        x['errorBalanceOrig'] = x.newbalanceOrig + x.amount - x.oldbalanceOrg\n",
    "        x['errorBalanceDest'] = x.oldbalanceDest + x.amount - x.newbalanceDest\n",
    "        \n",
    "        # log transform amount\n",
    "        eps=0.001 # 0 => 0.1Â¢\n",
    "        x['amount_log'] = np.log(x['amount'].values+eps)\n",
    "        \n",
    "        # reset the index and save.\n",
    "        x.reset_index(inplace=True)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classifier\"></a>\n",
    "## **a classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasSequentialModel(Sequential):\n",
    "    \"\"\"Generate a Simple Keras Model.\n",
    "        \n",
    "    This could be a very involved model builder, or it could\n",
    "    download a model and make it available for retraining,\n",
    "    `ala transfer learning`.\n",
    "    \n",
    "    So `model_args` might be number of layers, initializers,\n",
    "    or whole custom layers built elsewhere.\n",
    "    \n",
    "    :param model_args: Unused.\n",
    "    \n",
    "    Returns a keras sequential model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_args: dict={}):\n",
    "        clf = Sequential()\n",
    "        clf.add(Dense(units =15 , \n",
    "                kernel_initializer = 'uniform', \n",
    "                activation = 'relu', \n",
    "                input_dim = 11))\n",
    "        clf.add(Dense(units = 15, \n",
    "                kernel_initializer = 'uniform', \n",
    "                activation = 'relu'))\n",
    "        clf.add(Dense(units = 1, \n",
    "                kernel_initializer = 'uniform', \n",
    "                activation = 'sigmoid'))\n",
    "        clf.compile(optimizer = 'adam', \n",
    "                    loss = 'binary_crossentropy', \n",
    "                    metrics = ['accuracy'])\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>\n",
    "## **training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "    context: MLClientCtx,\n",
    "    feature_generator: TransformerMixin,\n",
    "    normalizer: TransformerMixin,\n",
    "    any_model: Any,\n",
    "    data: DataItem,\n",
    "    target_path: str = '',\n",
    "    name: str = 'model-default.pickle'\n",
    "    batch_size: int = 1024,\n",
    "    epochs: int = 5,\n",
    "    learning_rate: float = 0.1,\n",
    "    test_size: float = 0.1,\n",
    "    valid_size: float = 0.3,\n",
    "    verbose: bool = True,\n",
    "    random_state: int = 1,\n",
    "    ) -> None:\n",
    "    \"\"\"Train, validate, test and save a TF-Keras classifier model.\n",
    "    \n",
    "    :param context:             function context\n",
    "    :param feature_generator:   scikit learn transformer\n",
    "    :param normalizer:          scikit learn transformer\n",
    "    :param classidier_model:    keras Sequential model, could be any model\n",
    "    :param data:                complete raw input dataset\n",
    "    :param batch_size:          (1024) network feed batch size\n",
    "    :param epochs:              (5) training epochs\n",
    "    :param test_size:           (0.1) test set size as fraction\n",
    "    :param valid_size:          (0.3) validation set size as fraction\n",
    "    :param verbose:             (default True) Show metrics for \n",
    "                                training/validation steps\n",
    "    :param target_path:         destination for artifacts\n",
    "    :para name:                 model name\n",
    "        \n",
    "    Also included for demonstration are a randomly selected sample\n",
    "    of training parameters:\n",
    "    :param learning_rate: Step size at each iteration, constant.\n",
    "    \"\"\"\n",
    "    raw = functions.get_context_table(data)\n",
    "\n",
    "    train, test = train_test_split(raw, test_size=test_size)\n",
    "    train, valid = train_test_split(train, test_size=valid_size)\n",
    "    \n",
    "    y_train = train.pop('isFraud')\n",
    "    y_valid = valid.pop('isFraud')\n",
    "    y_test = test.pop('isFraud')\n",
    "\n",
    "    training_pipeline = [('features', feature_generator()),\n",
    "                          'scaler', normalizer()),\n",
    "                          'classifier', any_model()]\n",
    "\n",
    "    training_pipeline.fit(train, \n",
    "                          y_train, \n",
    "                          batch_size = batch_size, \n",
    "                          epochs = epochs)\n",
    "\n",
    "    y_pred = training_pipeline.predict(test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    context.log_result(\"accuracy\", float(acc))\n",
    "\n",
    "    functions.log_context_table(context, target_path, 'xtext.parquet', test)\n",
    "    functions.log_context_table(context, target_path, 'ytext.parquet', y_test)\n",
    "    functions.log_context_model(context, target_path, name, training_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _end of nuclio function definition_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"local-testing\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **testing your code locally**\n",
    "\n",
    "The function can be run locally and debugged/tested before deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import (mlconf,           # you can use mlconf for mlconf settings\n",
    "                  code_to_function,  # convert your code to a function\n",
    "                  new_function,      # create a new function, give it some code\n",
    "                  new_model_server,  # in this notebook create an inference server \n",
    "                  mount_v3io)        # use the v3io data fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set MLRun's DB path.  MLRun wil generate and store all of its tracking and metadata to the `MLRUN_DBATH` environment variable. **Please note that you should not be storing other data related to your experiments in this folder, let MLRun manage it.** We have set a `target_path` earlier in this notebook in the section [setting some source and destination paths](#paths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MLRUN_DBPATH=/User/mlrun\n",
    "mlconf.dbpath = \"/User/mlrun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = new_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc2parq_run = workflow.run(\n",
    "    name='acquire and store',\n",
    "    handler=functions.arc_to_parquet,\n",
    "    params={\n",
    "        'archive_url': '/User/projects/paysim/data/PS_20174392719_1491204439457_log.csv.zip',\n",
    "        'header': None,\n",
    "        'target_path': TARGET_PATH,\n",
    "        'name': 'paysim.parquet',\n",
    "        'chunksize': 10_000\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = workflow.run(\n",
    "    name='train and validate then store model',\n",
    "    handler=train,\n",
    "    inputs={\n",
    "        'data': arc2parq_run.outputs['data']\n",
    "    },\n",
    "    params={\n",
    "        'target_path': TARGET_PATH,\n",
    "        'srcname': 'paysim.parquet',\n",
    "        'feature_generator': FraudFeaturesGenerator,\n",
    "        'normalizer': StandardScaler,\n",
    "        'classifier_model': KerasSequentialModel,\n",
    "        'batch_size': 1024,\n",
    "        'epcohs': 5\n",
    "        'name': 'tfkeras-seq.pickle'\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"image\"></a>\n",
    "#### _Create a deployment image_\n",
    "\n",
    "Once debugged you can create a reusable image, and then deploy it for testing. In the following line we are converting the code block between the ```#nuclio: ignore``` and ```#nuclio: end-code``` to be run as a KubeJob.  Next we build an image named ```mlrun/mlrunlgb:latest```.  _**It is important to ensure that this image has been built at least once, and that you have access to it.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfkeras_job = code_to_function(runtime=\"job\").apply(mount_v3io())\n",
    "\n",
    "tfkeras_job.build(image=\"mlrun/mlruntfkeras:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While debugging, and _**after you have run**_ ```build``` **_at least once**_, you can comment out the last cell so that the build process isn\"t started needlessly.  The code can be injected into the job using the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_job.with_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipeline\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create a KubeFlow Pipeline from our functions**\n",
    "\n",
    "Our pipeline will consist of two instead of three steps, ```load``` and ```train```.  We\"ll drop the ```test```\n",
    "here since at the end of this deployment we can test the system with API requests.\n",
    "\n",
    "For complete details on KubeFlow Pipelines please refer to the following docs:\n",
    "1. **[KubeFlow pipelines](https://www.kubeflow.org/docs/pipelines/)**.\n",
    "2. **[kfp.dsl Python package](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html#module-kfp.dsl)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, the model server file name in the ```new_model_server``` function call below should identical in every respect to the name of the model server notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"TF-Keras Classifier Training Pipeline - Paysim\",\n",
    "    description=\"Shows how to use mlrun/kfp.\"\n",
    ")\n",
    "def tfkeras_pipeline(\n",
    "   learning_rate = [0.1, 0.3]\n",
    "):\n",
    "\n",
    "    acquire = tfkeras_job.as_step(\n",
    "        handler='my_funcs.file_functions.arc_to_parquet',\n",
    "        out_path=target_path, \n",
    "        params={\n",
    "        },\n",
    "        outputs=['data'])\n",
    "    \n",
    "    train = tfkeras_job.as_step(\n",
    "        handler='train ',\n",
    "        out_path=artifacts_path, \n",
    "        inputs={'model': train.outputs['model']},\n",
    "        outputs=['validation'])\n",
    "\n",
    "    # define a nuclio-serving function, generated from a notebook file\n",
    "    srvfn = new_model_server(\n",
    "        \"paysim-serving\", \n",
    "        model_class=\"TFKerasClassifier\", \n",
    "        filename=\"model-server.ipynb\")\n",
    "    \n",
    "    # deploy the model serving function with inputs from the training stage\n",
    "    deploy = srvfn.with_v3io(\"User\", \"~/\")\n",
    "    deploy = deploy.deploy_step(project=\"refactor-demos\", \n",
    "                                models={\"tfkeras_joblib\": train_step.outputs[\"model\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compile the pipeline\"></a>\n",
    "### _compile the pipeline_\n",
    "\n",
    "We can compile our KubeFlow pipeline and produce a yaml description of the pipeline worflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TARGET_PATH, exist_ok=True)\n",
    "kfp.compiler.Compiler().compile(lgbm_pipeline, os.path.join(TARGET_PATH, \"mlrunpipe.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(namespace=\"default-tenant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following line will run the pipeline as a job::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"learning_rate\": [ 0.1, 0.3]\n",
    "}\n",
    "\n",
    "run_result = client.create_run_from_pipeline_func(\n",
    "    tfkeras_pipeline, \n",
    "    arguments, \n",
    "    run_name=\"tfkeras_latest\",\n",
    "    experiment_name=\"tfkeras\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
