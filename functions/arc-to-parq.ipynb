{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rolling your own functions package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook we will develop a function that can be integrated, as is, into a machine learning pipeline.  MLRun offers many alternative options for packaging your code into reusable components, and we'll focus here on a common example--a function sitting inside a package module residing in a code repository, in this case, **[GitHub](https://github.com)**.\n",
    "\n",
    "### arc to parquet\n",
    "\n",
    "One of the most common steps of machine learning pipelines is the acquisition of remote archives.  In research and competitions these are often one-time downloads per project, whereas in commercial applications this can involve hundreds or even thousands of files per day.  In any case, if saving the data makes sense (cents!), then a popular format would be **[parquet](https://parquet.apache.org/documentation/latest/)**.  And that's because  paquet files can be loaded relatively quickly on systems with fast cpu's, it's columnar, it's compatible with **[Apache Arrow](https://arrow.apache.org/docs/python/parquet.html)**, it's a standard file format that comes with a certain guarantee (future proof) that it won't suffer versioning issues over time, and so on...\n",
    "\n",
    "Since we do alot of pipeline building with these raw data files, we would want this component to optionally make its data available to the next step in an MLRun pipeline, and not just store the data. So we log the data as an MLRun artifact.  This will also enable us to take a peek at the data in tabular format through an artifact viewer, and make available some descriptive stats and information on categorical types that may require further attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip uninstall -y mlrun\n",
    "    !pip install -U mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install -U joblib pandas pyarrow numpy==1.16.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    !pip install -U nuclio-jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio cmd -c pip install joblib pandas pyarrow numpy==1.16.4\n",
    "%nuclio cmd -c pip install mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from typing import (IO, \n",
    "                    AnyStr, \n",
    "                    TypeVar, \n",
    "                    Union, \n",
    "                    List, \n",
    "                    Tuple, Any)\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "from functions.tables import log_context_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_to_parquet(\n",
    "    context: MLClientCtx,\n",
    "    archive_url: Union[str, Path, IO[AnyStr]],\n",
    "    header: Union[None, List[str]] = None,\n",
    "    target_path: str = \"\",\n",
    "    name: str = \"\",\n",
    "    chunksize: int = 10_000,\n",
    "    log_data: bool = True,\n",
    "    key: str = 'raw_data'\n",
    ") -> None:\n",
    "    \"\"\"Open a file/object archive and save as a parquet file.\n",
    "    \n",
    "    Args:\n",
    "    :param context:     function context\n",
    "    :param archive_url: any valid string path consistent with the path variable\n",
    "                        of pandas.read_csv. ncluding strings as file paths, as urls, \n",
    "                        pathlib.Path objects, etc...\n",
    "    :param header:      column names\n",
    "    :param target_path: destination folder of table\n",
    "    :param name:        name file to be saved locally\n",
    "    :param chunksize:   (0) row size retrieved per iteration\n",
    "    :param log_data:    (True) if True, log the data so that it is available\n",
    "                        at the next step\n",
    "    :param key:         when logging data as an artifact, assign it to this\n",
    "                        key.\n",
    "    \"\"\"\n",
    "    os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "    if not name.endswith(\".parquet\"):\n",
    "        name += \".parquet\"\n",
    "\n",
    "    dest_path = os.path.join(target_path, name)\n",
    "\n",
    "    if not os.path.isfile(dest_path):\n",
    "        context.logger.info(\"destination file does not exist, downloading\")\n",
    "        pqwriter = None\n",
    "        for i, df in enumerate(\n",
    "            pd.read_csv(archive_url, chunksize=chunksize, names=header)\n",
    "        ):\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            if i == 0:\n",
    "                pqwriter = pq.ParquetWriter(dest_path, table.schema)\n",
    "            pqwriter.write_table(table)\n",
    "\n",
    "        if pqwriter:\n",
    "            pqwriter.close()\n",
    "\n",
    "        context.logger.info(f\"saved table to {dest_path}\")\n",
    "    else:\n",
    "        context.logger.info(\"destination file exists\")\n",
    "\n",
    "    if log_data:\n",
    "        context.logger.info(\"logging data to context\")\n",
    "        # we simply give the context our file location and \n",
    "        # assign it to `key`:\n",
    "        log_context_table(context, dest_path, key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
