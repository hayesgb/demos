{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Model - Inference Server Function\n",
    "________________________________________________________\n",
    "\n",
    "The function accepts a URL or binary image and provides an estimated binary-class predictionusing the tensorflow model developed in **[Deploy a tensorflow-horovod job as a pipeline](mlrun_mpijob_pipe.ipynb)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "# !pip install -U tensorflow==1.14.0 numpy==1.16.4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **install dependencies and set config**\n",
    "**Note**: In this demonstration since we pull tensorflow a Tensorflow 1.14 image directly from Docker Hub, we _**do not need to directly install it as a build command**_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio cmd -c pip install -U numpy==1.16.4 azure install keras requests pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.build.baseImage to 'tensorflow/tensorflow:1.14.0-py3'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config spec.build.baseImage = \"tensorflow/tensorflow:1.14.0-py3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **set function environment variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting 'IMAGE_WIDTH' environment variable\n",
      "%nuclio: setting 'IMAGE_HEIGHT' environment variable\n"
     ]
    }
   ],
   "source": [
    "%%nuclio env \n",
    "IMAGE_WIDTH=128\n",
    "IMAGE_HEIGHT=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **function code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img\n",
    "from os import environ, path\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from mlrun.execution import MLClientCtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Serving Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFModel(object):\n",
    "    def __init__(self,\n",
    "                 name: str, \n",
    "                 model_dir: str\n",
    "    ):\n",
    "        \"\"\"Model server\n",
    "        \n",
    "        :param name:      name of server\n",
    "        :param model_dir: destination folder of estimated model\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model_filepath = model_dir\n",
    "        self.model = None\n",
    "        self.ready = None\n",
    "\n",
    "        self.IMAGE_WIDTH = int(environ['IMAGE_WIDTH'])\n",
    "        self.IMAGE_HEIGHT = int(environ['IMAGE_HEIGHT'])\n",
    "        \n",
    "        try:\n",
    "            print(environ['classes_map'])\n",
    "            with open(environ['classes_map'], 'r') as f:\n",
    "                self.classes = json.load(f)\n",
    "        except:\n",
    "            self.classes = None\n",
    "        \n",
    "        print(f'Classes: {self.classes}')\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load serialized tensorflow model\n",
    "        \"\"\"\n",
    "        self.model = load_model(self.model_filepath)\n",
    "\n",
    "        self.ready = True\n",
    "\n",
    "    def _download_file(self, url, target_path):\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            with open(target_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        context:  MLClientCtx,\n",
    "        data\n",
    "    ):\n",
    "        \"\"\"Predict given data and estimated model\n",
    "        \n",
    "        :param context: MLClientCTx\n",
    "        :param data:    in this demonstation and byte array representing the image(s) submitted\n",
    "                        for classification\n",
    "        \"\"\"\n",
    "        img = Image.open(BytesIO(data))\n",
    "        img = img.resize((self.IMAGE_WIDTH, self.IMAGE_HEIGHT))\n",
    "\n",
    "        # Load image\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        images = np.vstack([x])\n",
    "\n",
    "        # Predict\n",
    "        predicted_probability = self.model.predict(images)\n",
    "\n",
    "        # return prediction\n",
    "        if self.classes:\n",
    "            predicted_classes = np.around(predicted_probability, 1).tolist()[0]\n",
    "            predicted_probabilities = predicted_probability.tolist()[0]\n",
    "            #print(predicted_classes)\n",
    "            #print(predicted_probabilities)\n",
    "            return {\n",
    "                'prediction': [self.classes[str(int(cls))] for cls in predicted_classes], \n",
    "                f'{self.classes[\"1\"]}-probability': predicted_probabilities\n",
    "            }\n",
    "        else:\n",
    "            return predicted_probability.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **routes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(context, model_name, event):\n",
    "    global models\n",
    "    global protocol\n",
    "\n",
    "    # Load the requested model\n",
    "    model = models[model_name]\n",
    "\n",
    "    # Verify model is loaded (Async)\n",
    "    if not model.ready:\n",
    "        model.load()\n",
    "    \n",
    "    # extract image data from event\n",
    "    try:\n",
    "        data = event.body\n",
    "        ctype = event.content_type\n",
    "        if not ctype or ctype.startswith('text/plain'):\n",
    "            # Get image from URL\n",
    "            url = data.decode('utf-8')\n",
    "            context.logger.debug_with('downloading image', url=url)\n",
    "            data = urlopen(url).read()\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise Exception(\"Failed to get data: {}\".format(e))                \n",
    "            \n",
    "    # Predict\n",
    "    results = model.predict(context, data)\n",
    "    context.logger.info(results)\n",
    "\n",
    "    # Wrap & return response\n",
    "    return context.Response(body=json.dumps(results),\n",
    "                            headers={},\n",
    "                            content_type='text/plain',\n",
    "                            status_code=200)\n",
    "\n",
    "# Router\n",
    "paths = {\n",
    "    'predict': predict,\n",
    "    'explain': '',\n",
    "    'outlier_detector': '',\n",
    "    'metrics': '',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = 'SERVING_MODEL_'\n",
    "models = {}\n",
    "\n",
    "def init_context(context):\n",
    "    global models\n",
    "    global model_prefix\n",
    "\n",
    "    # Initialize models from environment variables\n",
    "    # Using the {model_prefix}_{model_name} = {model_path} syntax\n",
    "    model_paths = {k[len(model_prefix):]: v for k, v in environ.items() if\n",
    "                   k.startswith(model_prefix)}\n",
    "\n",
    "    models = {name: TFModel(name=name, model_dir=path) for name, path in\n",
    "              model_paths.items()}\n",
    "    context.logger.info(f'Loaded {list(models.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_string = 'Got path: {}\\nPath must be <host>/<action>/<model-name> \\nactions: {} \\nmodels: {}'\n",
    "\n",
    "def handler(context, event):\n",
    "    global models\n",
    "    global paths\n",
    "\n",
    "    # check if valid route & model\n",
    "    sp_path = event.path.strip('/').split('/')\n",
    "    if len(sp_path) < 2 or sp_path[0] not in paths or sp_path[1] not in models:\n",
    "        return context.Response(body=err_string.format(event.path, '|'.join(paths), '|'.join(models.keys())),\n",
    "                                content_type='text/plain',\n",
    "                                status_code=400)\n",
    "        \n",
    "    function_path = sp_path[0] \n",
    "    model_name = sp_path[1]\n",
    "\n",
    "    context.logger.info(\n",
    "        f'Serving uri: {event.path} for route {function_path} '\n",
    "        f'with {model_name}, content type: {event.content_type}')\n",
    "\n",
    "    route = paths.get(function_path)\n",
    "    if route:\n",
    "        return route(context, model_name, event)\n",
    "\n",
    "    return context.Response(body='function {} not implemented'.format(function_path),\n",
    "                            content_type='text/plain',\n",
    "                            status_code=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **test locally**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your local TF / Keras version is the same as pulled in the nuclio image for accurate testing\n",
    "\n",
    "Set the served models and their file paths using: `SERVING_MODEL_<name> = <model file path>`\n",
    "\n",
    "> Note: this notebook assumes the model and categories are under <b>/User/mlrun/examples/</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-00f251db96db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SERVING_MODEL_cat_dog_v1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'models/cats_n_dogs.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classes_map'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'images/categories_map.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "environ['SERVING_MODEL_cat_dog_v1'] = base_dir + 'models/cats_n_dogs.h5'\n",
    "environ['classes_map'] = base_dir + 'images/categories_map.json'\n",
    "\n",
    "init_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cat_image_url = 'https://s3.amazonaws.com/iguazio-sample-data/images/catanddog/cat.102.jpg'\n",
    "response = requests.get(cat_image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)\n",
    "\n",
    "model_name = 'cat_dog_v1'\n",
    "event = nuclio.Event(body=response.content,\n",
    "                     content_type='image/jpeg',\n",
    "                     path=f'/predict/{model_name}')\n",
    "output = handler(context, event)\n",
    "print(str(output.body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **deploy the serving function to the cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the notebook code to deployable function, configure it\n",
    "from mlrun import code_to_function\n",
    "fn = code_to_function('tf-image-server-from-notebook', runtime='nuclio')\n",
    "\n",
    "# set the API/trigger, attach the home dir to the function\n",
    "fn.with_http(workers=2).add_volume('User','~/')\n",
    "\n",
    "# set the model file path SERVING_MODEL_<name> = <model file path>\n",
    "fn.set_env('SERVING_MODEL_cat_dog_v1', base_dir + 'models/cats_n_dogs.h5')\n",
    "fn.set_env('classes_map', base_dir + 'images/categories_map.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the function to the cluster\n",
    "addr = fn.deploy(project='nuclio-serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **test the function (jpeg url)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_image_url = 'https://s3.amazonaws.com/iguazio-sample-data/images/catanddog/dog.102.jpg'\n",
    "response = requests.get(dog_image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)\n",
    "\n",
    "headers = {'Content-type': 'text/plain'}\n",
    "response = requests.post(url=addr + f'/predict/{model_name}', data=dog_image_url, headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **test the function (jpeg image)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_image_url = 'https://s3.amazonaws.com/iguazio-sample-data/images/catanddog/cat.102.jpg'\n",
    "response = requests.get(cat_image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)\n",
    "\n",
    "headers = {'Content-type': 'image/jpeg'}\n",
    "response = requests.post(url=addr + f'/predict/{model_name}', data=response.content, headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
